{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_X39pQ5tVP4"
   },
   "source": [
    "# Train an agent to play FrozenLake using Q-learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ss-eZTqjte3O"
   },
   "source": [
    "[Gymnasium](https://gymnasium.farama.org) is a toolkit for developing and comparing reinforcement algorithms. It contains several test problem (*environments*) that have a shared interface, allowing you to write general algorithms. \n",
    "\n",
    "Let's start with importing all necessary dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%python` not found (But cell magic `%%python` exists, did you mean that instead?).\n"
     ]
    }
   ],
   "source": [
    "%python -m pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "K5LVzQUirmiF"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[classic-control] in /opt/homebrew/anaconda3/lib/python3.12/site-packages (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/homebrew/anaconda3/lib/python3.12/site-packages (from gymnasium[classic-control]) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/homebrew/anaconda3/lib/python3.12/site-packages (from gymnasium[classic-control]) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/homebrew/anaconda3/lib/python3.12/site-packages (from gymnasium[classic-control]) (4.11.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/homebrew/anaconda3/lib/python3.12/site-packages (from gymnasium[classic-control]) (0.0.4)\n",
      "Requirement already satisfied: pygame>=2.1.3 in /opt/homebrew/anaconda3/lib/python3.12/site-packages (from gymnasium[classic-control]) (2.6.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install 'gymnasium[classic-control]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WfP0PR7_tlLY"
   },
   "source": [
    "One of the environments Gym contains is FrozenLake.\n",
    "\n",
    "    Winter is here. You and your friends were tossing around a frisbee at the\n",
    "    park when you made a wild throw that left the frisbee out in the middle of\n",
    "    the lake. The water is mostly frozen, but there are a few holes where the\n",
    "    ice has melted. If you step into one of those holes, you'll fall into the\n",
    "    freezing water. At this time, there's an international frisbee shortage, so\n",
    "    it's absolutely imperative that you navigate across the lake and retrieve\n",
    "    the disc. However, the ice is slippery, so you won't always move in the\n",
    "    direction you intend.\n",
    "    The surface is described using a grid like the following\n",
    " \n",
    "        SFFF\n",
    "        FHFH\n",
    "        FFFH\n",
    "        HFFG\n",
    " \n",
    "    S : starting point, safe\n",
    "    F : frozen surface, safe\n",
    "    H : hole, fall to your doom\n",
    "    G : goal, where the frisbee is located\n",
    " \n",
    "    The episode ends when you reach the goal or fall in a hole.\n",
    "    You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
    "\n",
    "More info on: https://gymnasium.farama.org/environments/toy_text/frozen_lake/\n",
    "\n",
    "Let's start and create the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "V90ySjfLtynA"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cgsfBm4At2Lg"
   },
   "source": [
    "## Initialize the Q-table "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6lM_cuat-9q"
   },
   "source": [
    "Remember: \n",
    "- number of rows: number of states\n",
    "- number of columns: number of actions\n",
    "q-table: np array of dimension (states, actions)\n",
    "\n",
    "add some things on how to make np arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WcMatYVCuXM1"
   },
   "outputs": [],
   "source": [
    "state_space_size = env.observation_space.n\n",
    "action_space_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "O7Rr1yo1ujag"
   },
   "outputs": [],
   "source": [
    "q_table = np.zeros((state_space_size, action_space_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1143,
     "status": "ok",
     "timestamp": 1618321123871,
     "user": {
      "displayName": "Marjon Blondeel",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1bZUZhj06leL74e2Mn-eu1NyztKgDXIMJdj6RgQ=s64",
      "userId": "07230704137495875448"
     },
     "user_tz": -120
    },
    "id": "dbuj_-HCunZu",
    "outputId": "e1360b6e-180b-47c1-e7a9-47abe8530f38"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0iZkcf_1vL3Y"
   },
   "source": [
    "## Set the hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zot-2RUGvOl9"
   },
   "source": [
    "In this section we will initialize the hyperparameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Booh7w41vfLw"
   },
   "source": [
    "The first one is the *discount factor*. This is a number in [0,1] indicating how much the agent cares about rewards in the future relative to those in the immediate future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "liMcepGyw3uW"
   },
   "outputs": [],
   "source": [
    "discount_factor = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CthvEAPRy0bg"
   },
   "source": [
    "The second parameter is the *learning rate*. This is a number in [0,1] indicating how quickly the agent will adopt the new (learned) Q-value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "GiAiP3b0zRnr"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATMjtQ1wzZT-"
   },
   "source": [
    "Finally we set the necessary parameters to deal with the trade-off between exploration and exploitation. We have to set\n",
    "- *initial exploration rate*: upper bound for the exploration rate and initial exploration rate, we will use this to update the exploration rate\n",
    "- *minimum exploration rate*: lower bound for the exploration rate, by setting it to a value greater than 0, we make sure there is always a probability for exploration\n",
    "- *exploration rate*: probability that the agent will explore, will be updated after each episode, using exponential decay: \n",
    "$$N(t) = N_0 e^{-\\lambda t}$$\n",
    "where $N_0$ is the initial quantity, $N(t)$ is the quantity at time step $t$ and $\\lambda$ is the rate of decay.\n",
    "- *exploration rate decay*: how fast or slow does the exploration rate decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "I9qm0lw2-sn9"
   },
   "outputs": [],
   "source": [
    "initial_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_rate = 1\n",
    "exploration_decay_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1eYlK0-c-45x"
   },
   "source": [
    "We also set number of episodes and maximum number of steps per episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "DFurhi73_AsW"
   },
   "outputs": [],
   "source": [
    "num_episodes = 10000\n",
    "max_steps_per_episode = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aaA0V6O9_TzJ"
   },
   "source": [
    "## Train the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3SrazgwEAaN5"
   },
   "source": [
    "Store the total rewards for each episode. This is for diagnostics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Mmxraf1dekU"
   },
   "source": [
    "If an episode is succesfull this translate to reward 1 if not to reward 0, so we can use this array to check % of successes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "XuCPM2laARJs"
   },
   "outputs": [],
   "source": [
    "rewards = np.zeros(num_episodes) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zy0yUfNqCHP8"
   },
   "source": [
    "In each episode:\n",
    "1.   Reset environment\n",
    "2.   For each step in the episode:\n",
    "  - pick an action $a$ by generating a random float $r$ in $[0,1]$\n",
    "    - if $r > \\epsilon$ we choose the next action by exploitation\n",
    "    - if $r \\leq \\epsilon$ we choose the next action by exploration\n",
    "  - take action $a$ and observe reward $R$ and next state $s'$\n",
    "  - update Q-table \n",
    "  $$ q(s,a) = q(s,a) + \\alpha(R + \\gamma max_{a'} q(s', a') - q(s,a)) $$\n",
    "3. decrease exploration rate proportional to its current value, we will use exponential decay: \n",
    "$$N(t) = N_0 e^{-\\lambda t}$$\n",
    "where $N_0$ is the initial quantity and $N(t)$ is the quantity at time step $t$.\n",
    "\n",
    "\n",
    "\n",
    "with \n",
    "- $\\epsilon$ the exploration rate,\n",
    "- $\\lambda$ the exploration rate decay,\n",
    "- $\\alpha$ the learning rate and \n",
    "- $\\gamma$ the discount factor.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "zOvIxI_A_V9J"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.12/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "for episode in range(num_episodes):\n",
    "    state = env.reset()[0] # start with a clean slate\n",
    "    reward_episode = 0 # keep track of total reward for this episode\n",
    "\n",
    "    for step in range(max_steps_per_episode):\n",
    "        # exploration or exploitation? -> we need to pick a random float in 0 and 1\n",
    "        exploration_rate_treshold = random.uniform(0,1)\n",
    "        if exploration_rate_treshold > exploration_rate:\n",
    "            action = np.argmax(q_table[state]) #exploitation: look in row \"state\" which column \"action\" has the highest value\n",
    "        else:\n",
    "            action = env.action_space.sample() #exploration: random action\n",
    "\n",
    "        # take a step in the environment: let's store the new state under a new name since we still need to use the old state\n",
    "        new_state, reward, terminated, truncated , info = env.step(action)\n",
    "        \n",
    "        # terminated = True if environment terminates (eg. due to task completion, failure etc.)\n",
    "        # truncated = True if episode truncates due to a time limit or a reason that is not defined as part of the task.\n",
    "        done = truncated or terminated \n",
    "\n",
    "        # update q-table for current (state, action) pair\n",
    "        q_table[state, action] += learning_rate *\\\n",
    "         (reward + discount_factor * np.max(q_table[new_state]) - q_table[state, action])\n",
    "\n",
    "        # update state\n",
    "        state = new_state\n",
    "        # update reward episode\n",
    "        reward_episode += reward # rewards[episode] += reward\n",
    "\n",
    "        # check if episode ended\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # add reward current episode to rewards array\n",
    "    rewards[episode] = reward_episode\n",
    "\n",
    "    # update exploration rate\n",
    "    exploration_rate = max(initial_exploration_rate * np.exp(- exploration_decay_rate * episode), min_exploration_rate)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IsE0vf_ArDFX"
   },
   "source": [
    "Let's print the average reward per thousand episodes, this way we can get an idea about how rewards have changed over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "KduAhbZ3ZuAI"
   },
   "outputs": [],
   "source": [
    "rewards_per_thousand_episodes = np.split(rewards, num_episodes/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after  1000  episodes:  0.0\n",
      "after  2000  episodes:  0.0\n",
      "after  3000  episodes:  0.0\n",
      "after  4000  episodes:  0.0\n",
      "after  5000  episodes:  0.0\n",
      "after  6000  episodes:  0.0\n",
      "after  7000  episodes:  0.0\n",
      "after  8000  episodes:  0.0\n",
      "after  9000  episodes:  0.0\n",
      "after  10000  episodes:  0.0\n"
     ]
    }
   ],
   "source": [
    "count = 1000\n",
    "for reward_group in rewards_per_thousand_episodes:\n",
    "    print(\"after \", count, \" episodes: \", np.average(reward_group))\n",
    "    count+=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THvt3Zx5_O-X"
   },
   "source": [
    "Let's also visualize the *learning curve* by plotting the moving average using a window length of 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BnJK6G4Z-9Yg"
   },
   "outputs": [],
   "source": [
    "def moving_average(array, window):\n",
    "    return np.convolve(array, np.ones(window), 'valid') / window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 800,
     "status": "ok",
     "timestamp": 1618321543335,
     "user": {
      "displayName": "Marjon Blondeel",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1bZUZhj06leL74e2Mn-eu1NyztKgDXIMJdj6RgQ=s64",
      "userId": "07230704137495875448"
     },
     "user_tz": -120
    },
    "id": "C-npvt2E_HIY",
    "outputId": "669283d5-cee7-4a03-a96b-8d2cb7997085"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moving_average(rewards, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "executionInfo": {
     "elapsed": 778,
     "status": "ok",
     "timestamp": 1618321555999,
     "user": {
      "displayName": "Marjon Blondeel",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1bZUZhj06leL74e2Mn-eu1NyztKgDXIMJdj6RgQ=s64",
      "userId": "07230704137495875448"
     },
     "user_tz": -120
    },
    "id": "ZYCLeHCm_YwY",
    "outputId": "01d176c1-b242-42f3-8a7b-3631c5a9442b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjgAAAGdCAYAAAAfTAk2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAgfUlEQVR4nO3de3BU5f3H8c9CYAOarECaLJEAwToCjbaY1JjUiFQNd6TSFkGitpaaKoaQsXKzA8VKhDqUYSJQKdo6KjAOYGknzRCqptgst5AgUMReUkIhawTDbhRNuDy/Pxj213VDSKy7SR7er5n9I89+z+45OaN5z9kLDmOMEQAAgEW6tPcOAAAAfNkIHAAAYB0CBwAAWIfAAQAA1iFwAACAdQgcAABgHQIHAABYh8ABAADWiWrvHWgP58+f1/HjxxUTEyOHw9HeuwMAAFrBGKOGhgYlJiaqS5eWr9FckYFz/PhxJSUltfduAACAL+Do0aPq169fizNXZODExMRIuvALio2Nbee9AQAAreH3+5WUlBT4O96SKzJwLr4sFRsbS+AAANDJtObtJbzJGAAAWIfAAQAA1iFwAACAdQgcAABgHQIHAABYh8ABAADWIXAAAIB1CBwAAGAdAgcAAFiHwAEAANYhcAAAgHUIHAAAYB0CBwAAWIfAAQAA1iFwAACAdQgcAABgHQIHAABYh8ABAADWIXAAAIB1CBwAAGAdAgcAAFiHwAEAANYhcAAAgHUIHAAAYB0CBwAAWIfAAQAA1iFwAACAdQgcAABgHQIHAABYh8ABAADWIXAAAIB1CBwAAGAdAgcAAFiHwAEAANYhcAAAgHUIHAAAYB0CBwAAWIfAAQAA1iFwAACAdQgcAABgHQIHAABYh8ABAADWIXAAAIB1IhI4K1euVHJysqKjo5Wamqrt27e3OF9WVqbU1FRFR0dr0KBBWr169SVn169fL4fDoYkTJ37Jew0AADqrsAfOhg0blJ+fr/nz56uyslJZWVkaPXq0ampqmp2vrq7WmDFjlJWVpcrKSs2bN095eXnauHFjyOyRI0f0xBNPKCsrK9yHAQAAOhGHMcaE8wnS09N18803a9WqVYG1IUOGaOLEiSosLAyZnz17trZs2aJDhw4F1nJzc7Vv3z55PJ7A2rlz5zR8+HD94Ac/0Pbt23Xq1Cm98cYbrdonv98vl8sln8+n2NjYL35wAAAgYtry9zusV3CamppUUVGh7OzsoPXs7GyVl5c3u43H4wmZHzlypPbs2aMzZ84E1hYtWqSvfOUrevjhhy+7H42NjfL7/UE3AABgr7AGzokTJ3Tu3DklJCQErSckJMjr9Ta7jdfrbXb+7NmzOnHihCTpr3/9q9auXas1a9a0aj8KCwvlcrkCt6SkpC9wNAAAoLOIyJuMHQ5H0M/GmJC1y81fXG9oaNC0adO0Zs0axcXFter5586dK5/PF7gdPXq0jUcAAAA6k6hwPnhcXJy6du0acrWmrq4u5CrNRW63u9n5qKgo9enTRwcPHtS///1vjR8/PnD/+fPnJUlRUVE6fPiwrrvuuqDtnU6nnE7nl3FIAACgEwjrFZzu3bsrNTVVpaWlQeulpaXKzMxsdpuMjIyQ+a1btyotLU3dunXT4MGDtX//flVVVQVuEyZM0IgRI1RVVcXLTwAAILxXcCSpoKBAOTk5SktLU0ZGhl544QXV1NQoNzdX0oWXj44dO6aXX35Z0oVPTBUVFamgoEDTp0+Xx+PR2rVrtW7dOklSdHS0UlJSgp7jmmuukaSQdQAAcGUKe+BMnjxZJ0+e1KJFi1RbW6uUlBQVFxdrwIABkqTa2tqg78RJTk5WcXGxZs2apeeff16JiYlasWKFJk2aFO5dBQAAlgj79+B0RHwPDgAAnU+H+R4cAACA9kDgAAAA6xA4AADAOgQOAACwDoEDAACsQ+AAAADrEDgAAMA6BA4AALAOgQMAAKxD4AAAAOsQOAAAwDoEDgAAsA6BAwAArEPgAAAA6xA4AADAOgQOAACwDoEDAACsQ+AAAADrEDgAAMA6BA4AALAOgQMAAKxD4AAAAOsQOAAAwDoEDgAAsA6BAwAArEPgAAAA6xA4AADAOgQOAACwDoEDAACsQ+AAAADrEDgAAMA6BA4AALAOgQMAAKxD4AAAAOsQOAAAwDoEDgAAsA6BAwAArEPgAAAA6xA4AADAOgQOAACwDoEDAACsQ+AAAADrEDgAAMA6BA4AALAOgQMAAKxD4AAAAOsQOAAAwDoEDgAAsA6BAwAArEPgAAAA6xA4AADAOgQOAACwDoEDAACsQ+AAAADrEDgAAMA6BA4AALBORAJn5cqVSk5OVnR0tFJTU7V9+/YW58vKypSamqro6GgNGjRIq1evDrp/zZo1ysrKUq9evdSrVy/ddddd2rVrVzgPAQAAdCJhD5wNGzYoPz9f8+fPV2VlpbKysjR69GjV1NQ0O19dXa0xY8YoKytLlZWVmjdvnvLy8rRx48bAzNtvv60pU6borbfeksfjUf/+/ZWdna1jx46F+3AAAEAn4DDGmHA+QXp6um6++WatWrUqsDZkyBBNnDhRhYWFIfOzZ8/Wli1bdOjQocBabm6u9u3bJ4/H0+xznDt3Tr169VJRUZEeeOCBy+6T3++Xy+WSz+dTbGzsFzgqAAAQaW35+x3WKzhNTU2qqKhQdnZ20Hp2drbKy8ub3cbj8YTMjxw5Unv27NGZM2ea3eb06dM6c+aMevfu3ez9jY2N8vv9QTcAAGCvsAbOiRMndO7cOSUkJAStJyQkyOv1NruN1+ttdv7s2bM6ceJEs9vMmTNH1157re66665m7y8sLJTL5QrckpKSvsDRAACAziIibzJ2OBxBPxtjQtYuN9/cuiQtXbpU69at06ZNmxQdHd3s482dO1c+ny9wO3r0aFsPAQAAdCJR4XzwuLg4de3aNeRqTV1dXchVmovcbnez81FRUerTp0/Q+nPPPafFixdr27Ztuummmy65H06nU06n8wseBQAA6GzCegWne/fuSk1NVWlpadB6aWmpMjMzm90mIyMjZH7r1q1KS0tTt27dAmu//OUv9fTTT6ukpERpaWlf/s4DAIBOK+wvURUUFOg3v/mNXnzxRR06dEizZs1STU2NcnNzJV14+ei/P/mUm5urI0eOqKCgQIcOHdKLL76otWvX6oknngjMLF26VE899ZRefPFFDRw4UF6vV16vVx9//HG4DwcAAHQCYX2JSpImT56skydPatGiRaqtrVVKSoqKi4s1YMAASVJtbW3Qd+IkJyeruLhYs2bN0vPPP6/ExEStWLFCkyZNCsysXLlSTU1N+u53vxv0XAsWLNDChQvDfUgAAKCDC/v34HREfA8OAACdT4f5HhwAAID2QOAAAADrEDgAAMA6BA4AALAOgQMAAKxD4AAAAOsQOAAAwDoEDgAAsA6BAwAArEPgAAAA6xA4AADAOgQOAACwDoEDAACsQ+AAAADrEDgAAMA6BA4AALAOgQMAAKxD4AAAAOsQOAAAwDoEDgAAsA6BAwAArEPgAAAA6xA4AADAOgQOAACwDoEDAACsQ+AAAADrEDgAAMA6BA4AALAOgQMAAKxD4AAAAOsQOAAAwDoEDgAAsA6BAwAArEPgAAAA6xA4AADAOgQOAACwDoEDAACsQ+AAAADrEDgAAMA6BA4AALAOgQMAAKxD4AAAAOsQOAAAwDoEDgAAsA6BAwAArEPgAAAA6xA4AADAOgQOAACwDoEDAACsQ+AAAADrEDgAAMA6BA4AALAOgQMAAKxD4AAAAOsQOAAAwDoEDgAAsE5EAmflypVKTk5WdHS0UlNTtX379hbny8rKlJqaqujoaA0aNEirV68Omdm4caOGDh0qp9OpoUOHavPmzeHafQAA0MmEPXA2bNig/Px8zZ8/X5WVlcrKytLo0aNVU1PT7Hx1dbXGjBmjrKwsVVZWat68ecrLy9PGjRsDMx6PR5MnT1ZOTo727dunnJwcff/739fOnTvDfTgAAKATcBhjTDifID09XTfffLNWrVoVWBsyZIgmTpyowsLCkPnZs2dry5YtOnToUGAtNzdX+/btk8fjkSRNnjxZfr9ff/rTnwIzo0aNUq9evbRu3brL7pPf75fL5ZLP51NsbOz/cngAACBC2vL3OyqcO9LU1KSKigrNmTMnaD07O1vl5eXNbuPxeJSdnR20NnLkSK1du1ZnzpxRt27d5PF4NGvWrJCZ5cuXN/uYjY2NamxsDPzs9/u/wNFc3omPG/X8W/8Iy2MDANCZxF3t1GMjvtpuzx/WwDlx4oTOnTunhISEoPWEhAR5vd5mt/F6vc3Onz17VidOnFDfvn0vOXOpxywsLNTPf/7z/+FIWsf/6Rm99Nd/h/15AADo6AZ95Sp7A+cih8MR9LMxJmTtcvOfX2/LY86dO1cFBQWBn/1+v5KSklq3821wTc/uemzEdV/64wIA0Nn06tm9XZ8/rIETFxenrl27hlxZqaurC7kCc5Hb7W52PioqSn369Glx5lKP6XQ65XQ6v+hhtFrvq7rrpyMHh/15AABAy8L6Karu3bsrNTVVpaWlQeulpaXKzMxsdpuMjIyQ+a1btyotLU3dunVrceZSjwkAAK4sYX+JqqCgQDk5OUpLS1NGRoZeeOEF1dTUKDc3V9KFl4+OHTuml19+WdKFT0wVFRWpoKBA06dPl8fj0dq1a4M+HTVz5kzdfvvtWrJkie655x79/ve/17Zt2/TOO++E+3AAAEAnEPbAmTx5sk6ePKlFixaptrZWKSkpKi4u1oABAyRJtbW1Qd+Jk5ycrOLiYs2aNUvPP/+8EhMTtWLFCk2aNCkwk5mZqfXr1+upp57Sz372M1133XXasGGD0tPTw304AACgEwj79+B0RHwPDgAAnU9b/n7zb1EBAADrEDgAAMA6BA4AALAOgQMAAKxD4AAAAOsQOAAAwDoEDgAAsA6BAwAArEPgAAAA6xA4AADAOgQOAACwDoEDAACsQ+AAAADrEDgAAMA6BA4AALAOgQMAAKxD4AAAAOsQOAAAwDoEDgAAsA6BAwAArEPgAAAA6xA4AADAOgQOAACwDoEDAACsQ+AAAADrEDgAAMA6BA4AALAOgQMAAKxD4AAAAOsQOAAAwDoEDgAAsA6BAwAArEPgAAAA6xA4AADAOgQOAACwDoEDAACsQ+AAAADrEDgAAMA6BA4AALAOgQMAAKxD4AAAAOsQOAAAwDoEDgAAsA6BAwAArEPgAAAA6xA4AADAOgQOAACwDoEDAACsQ+AAAADrEDgAAMA6BA4AALAOgQMAAKxD4AAAAOsQOAAAwDoEDgAAsA6BAwAArBPWwKmvr1dOTo5cLpdcLpdycnJ06tSpFrcxxmjhwoVKTExUjx49dMcdd+jgwYOB+z/66CM9/vjjuuGGG9SzZ0/1799feXl58vl84TwUAADQiYQ1cKZOnaqqqiqVlJSopKREVVVVysnJaXGbpUuXatmyZSoqKtLu3bvldrt19913q6GhQZJ0/PhxHT9+XM8995z279+v3/72tyopKdHDDz8czkMBAACdiMMYY8LxwIcOHdLQoUO1Y8cOpaenS5J27NihjIwMvffee7rhhhtCtjHGKDExUfn5+Zo9e7YkqbGxUQkJCVqyZIkeeeSRZp/r9ddf17Rp0/TJJ58oKirqsvvm9/vlcrnk8/kUGxv7PxwlAACIlLb8/Q7bFRyPxyOXyxWIG0m69dZb5XK5VF5e3uw21dXV8nq9ys7ODqw5nU4NHz78kttIChxoa+IGAADYL2xF4PV6FR8fH7IeHx8vr9d7yW0kKSEhIWg9ISFBR44caXabkydP6umnn77k1R3pwlWgxsbGwM9+v/+y+w8AADqvNl/BWbhwoRwOR4u3PXv2SJIcDkfI9saYZtf/2+fvv9Q2fr9fY8eO1dChQ7VgwYJLPl5hYWHgjc4ul0tJSUmtOVQAANBJtfkKzowZM3Tfffe1ODNw4EC9++67+uCDD0Lu+/DDD0Ou0FzkdrslXbiS07dv38B6XV1dyDYNDQ0aNWqUrr76am3evFndunW75P7MnTtXBQUFgZ/9fj+RAwCAxdocOHFxcYqLi7vsXEZGhnw+n3bt2qVbbrlFkrRz5075fD5lZmY2u01ycrLcbrdKS0s1bNgwSVJTU5PKysq0ZMmSwJzf79fIkSPldDq1ZcsWRUdHt7gvTqdTTqeztYcIAAA6ubC9yXjIkCEaNWqUpk+frh07dmjHjh2aPn26xo0bF/QJqsGDB2vz5s2SLrw0lZ+fr8WLF2vz5s06cOCAHnroIfXs2VNTp06VdOHKTXZ2tj755BOtXbtWfr9fXq9XXq9X586dC9fhAACATiSsHzt69dVXlZeXF/hU1IQJE1RUVBQ0c/jw4aAv6XvyySf16aef6tFHH1V9fb3S09O1detWxcTESJIqKiq0c+dOSdJXv/rVoMeqrq7WwIEDw3hEAACgMwjb9+B0ZHwPDgAAnU+H+B4cAACA9kLgAAAA6xA4AADAOgQOAACwDoEDAACsQ+AAAADrEDgAAMA6BA4AALAOgQMAAKxD4AAAAOsQOAAAwDoEDgAAsA6BAwAArEPgAAAA6xA4AADAOgQOAACwDoEDAACsQ+AAAADrEDgAAMA6BA4AALAOgQMAAKxD4AAAAOsQOAAAwDoEDgAAsA6BAwAArEPgAAAA6xA4AADAOgQOAACwDoEDAACsQ+AAAADrEDgAAMA6BA4AALAOgQMAAKxD4AAAAOsQOAAAwDoEDgAAsA6BAwAArEPgAAAA6xA4AADAOgQOAACwDoEDAACsQ+AAAADrEDgAAMA6BA4AALAOgQMAAKxD4AAAAOsQOAAAwDoEDgAAsA6BAwAArEPgAAAA6xA4AADAOgQOAACwDoEDAACsQ+AAAADrEDgAAMA6BA4AALAOgQMAAKwT1sCpr69XTk6OXC6XXC6XcnJydOrUqRa3McZo4cKFSkxMVI8ePXTHHXfo4MGDl5wdPXq0HA6H3njjjS//AAAAQKcU1sCZOnWqqqqqVFJSopKSElVVVSknJ6fFbZYuXaply5apqKhIu3fvltvt1t13362GhoaQ2eXLl8vhcIRr9wEAQCcVFa4HPnTokEpKSrRjxw6lp6dLktasWaOMjAwdPnxYN9xwQ8g2xhgtX75c8+fP17333itJ+t3vfqeEhAS99tpreuSRRwKz+/bt07Jly7R792717ds3XIcBAAA6obBdwfF4PHK5XIG4kaRbb71VLpdL5eXlzW5TXV0tr9er7OzswJrT6dTw4cODtjl9+rSmTJmioqIiud3uy+5LY2Oj/H5/0A0AANgrbIHj9XoVHx8fsh4fHy+v13vJbSQpISEhaD0hISFom1mzZikzM1P33HNPq/alsLAw8D4gl8ulpKSk1h4GAADohNocOAsXLpTD4WjxtmfPHklq9v0xxpjLvm/m8/f/9zZbtmzRm2++qeXLl7d6n+fOnSufzxe4HT16tNXbAgCAzqfN78GZMWOG7rvvvhZnBg4cqHfffVcffPBByH0ffvhhyBWaiy6+3OT1eoPeV1NXVxfY5s0339Q///lPXXPNNUHbTpo0SVlZWXr77bdDHtfpdMrpdLa4zwAAwB5tDpy4uDjFxcVddi4jI0M+n0+7du3SLbfcIknauXOnfD6fMjMzm90mOTlZbrdbpaWlGjZsmCSpqalJZWVlWrJkiSRpzpw5+tGPfhS03Y033qhf/epXGj9+fFsPBwAAWChsn6IaMmSIRo0apenTp+vXv/61JOnHP/6xxo0bF/QJqsGDB6uwsFDf+c535HA4lJ+fr8WLF+v666/X9ddfr8WLF6tnz56aOnWqpAtXeZp7Y3H//v2VnJwcrsMBAACdSNgCR5JeffVV5eXlBT4VNWHCBBUVFQXNHD58WD6fL/Dzk08+qU8//VSPPvqo6uvrlZ6erq1btyomJiacuwoAACziMMaY9t6JSPP7/XK5XPL5fIqNjW3v3QEAAK3Qlr/f/FtUAADAOgQOAACwDoEDAACsQ+AAAADrEDgAAMA6BA4AALAOgQMAAKxD4AAAAOsQOAAAwDoEDgAAsA6BAwAArEPgAAAA6xA4AADAOgQOAACwDoEDAACsQ+AAAADrEDgAAMA6BA4AALAOgQMAAKxD4AAAAOsQOAAAwDoEDgAAsA6BAwAArEPgAAAA6xA4AADAOgQOAACwDoEDAACsQ+AAAADrEDgAAMA6BA4AALAOgQMAAKxD4AAAAOsQOAAAwDoEDgAAsA6BAwAArEPgAAAA6xA4AADAOgQOAACwDoEDAACsQ+AAAADrEDgAAMA6BA4AALBOVHvvQHswxkiS/H5/O+8JAABorYt/ty/+HW/JFRk4DQ0NkqSkpKR23hMAANBWDQ0NcrlcLc44TGsyyDLnz5/X8ePHFRMTI4fD0d670yH5/X4lJSXp6NGjio2Nbe/dueJxPjoWzkfHwznpWMJ1PowxamhoUGJiorp0afldNlfkFZwuXbqoX79+7b0bnUJsbCz/s+hAOB8dC+ej4+GcdCzhOB+Xu3JzEW8yBgAA1iFwAACAdQgcNMvpdGrBggVyOp3tvSsQ56Oj4Xx0PJyTjqUjnI8r8k3GAADAblzBAQAA1iFwAACAdQgcAABgHQIHAABYh8CxVGFhob75zW8qJiZG8fHxmjhxog4fPhw0Y4zRwoULlZiYqB49euiOO+7QwYMHg2YaGxv1+OOPKy4uTldddZUmTJig//znP0Ez9fX1ysnJkcvlksvlUk5Ojk6dOhXuQ+zUCgsL5XA4lJ+fH1jjfETesWPHNG3aNPXp00c9e/bUN77xDVVUVATu55xEztmzZ/XUU08pOTlZPXr00KBBg7Ro0SKdP38+MMP5CJ+//OUvGj9+vBITE+VwOPTGG28E3R/J331NTY3Gjx+vq666SnFxccrLy1NTU1PbD8rASiNHjjQvvfSSOXDggKmqqjJjx441/fv3Nx9//HFg5tlnnzUxMTFm48aNZv/+/Wby5Mmmb9++xu/3B2Zyc3PNtddea0pLS83evXvNiBEjzNe//nVz9uzZwMyoUaNMSkqKKS8vN+Xl5SYlJcWMGzcuosfbmezatcsMHDjQ3HTTTWbmzJmBdc5HZH300UdmwIAB5qGHHjI7d+401dXVZtu2beYf//hHYIZzEjm/+MUvTJ8+fcwf//hHU11dbV5//XVz9dVXm+XLlwdmOB/hU1xcbObPn282btxoJJnNmzcH3R+p3/3Zs2dNSkqKGTFihNm7d68pLS01iYmJZsaMGW0+JgLnClFXV2ckmbKyMmOMMefPnzdut9s8++yzgZnPPvvMuFwus3r1amOMMadOnTLdunUz69evD8wcO3bMdOnSxZSUlBhjjPnb3/5mJJkdO3YEZjwej5Fk3nvvvUgcWqfS0NBgrr/+elNaWmqGDx8eCBzOR+TNnj3b3HbbbZe8n3MSWWPHjjU//OEPg9buvfdeM23aNGMM5yOSPh84kfzdFxcXmy5duphjx44FZtatW2ecTqfx+XxtOg5eorpC+Hw+SVLv3r0lSdXV1fJ6vcrOzg7MOJ1ODR8+XOXl5ZKkiooKnTlzJmgmMTFRKSkpgRmPxyOXy6X09PTAzK233iqXyxWYwf977LHHNHbsWN11111B65yPyNuyZYvS0tL0ve99T/Hx8Ro2bJjWrFkTuJ9zElm33Xab/vznP+v999+XJO3bt0/vvPOOxowZI4nz0Z4i+bv3eDxKSUlRYmJiYGbkyJFqbGwMevm4Na7If2zzSmOMUUFBgW677TalpKRIkrxeryQpISEhaDYhIUFHjhwJzHTv3l29evUKmbm4vdfrVXx8fMhzxsfHB2Zwwfr167V3717t3r075D7OR+T961//0qpVq1RQUKB58+Zp165dysvLk9Pp1AMPPMA5ibDZs2fL5/Np8ODB6tq1q86dO6dnnnlGU6ZMkcR/I+0pkr97r9cb8jy9evVS9+7d23x+CJwrwIwZM/Tuu+/qnXfeCbnP4XAE/WyMCVn7vM/PNDffmse5khw9elQzZ87U1q1bFR0dfck5zkfknD9/XmlpaVq8eLEkadiwYTp48KBWrVqlBx54IDDHOYmMDRs26JVXXtFrr72mr33ta6qqqlJ+fr4SExP14IMPBuY4H+0nUr/7L+v88BKV5R5//HFt2bJFb731lvr16xdYd7vdkhRSxHV1dYF6drvdampqUn19fYszH3zwQcjzfvjhhyEVfiWrqKhQXV2dUlNTFRUVpaioKJWVlWnFihWKiooK/K44H5HTt29fDR06NGhtyJAhqqmpkcR/I5H205/+VHPmzNF9992nG2+8UTk5OZo1a5YKCwslcT7aUyR/9263O+R56uvrdebMmTafHwLHUsYYzZgxQ5s2bdKbb76p5OTkoPuTk5PldrtVWloaWGtqalJZWZkyMzMlSampqerWrVvQTG1trQ4cOBCYycjIkM/n065duwIzO3fulM/nC8xAuvPOO7V//35VVVUFbmlpabr//vtVVVWlQYMGcT4i7Fvf+lbIVye8//77GjBggCT+G4m006dPq0uX4D9JXbt2DXxMnPPRfiL5u8/IyNCBAwdUW1sbmNm6daucTqdSU1PbtuNteksyOo2f/OQnxuVymbffftvU1tYGbqdPnw7MPPvss8blcplNmzaZ/fv3mylTpjT7sb9+/fqZbdu2mb1795pvf/vbzX7s76abbjIej8d4PB5z4403XvEfuWyN//4UlTGcj0jbtWuXiYqKMs8884z5+9//bl599VXTs2dP88orrwRmOCeR8+CDD5prr7028DHxTZs2mbi4OPPkk08GZjgf4dPQ0GAqKytNZWWlkWSWLVtmKisrzZEjR4wxkfvdX/yY+J133mn27t1rtm3bZvr168fHxPH/JDV7e+mllwIz58+fNwsWLDBut9s4nU5z++23m/379wc9zqeffmpmzJhhevfubXr06GHGjRtnampqgmZOnjxp7r//fhMTE2NiYmLM/fffb+rr6yNwlJ3b5wOH8xF5f/jDH0xKSopxOp1m8ODB5oUXXgi6n3MSOX6/38ycOdP079/fREdHm0GDBpn58+ebxsbGwAznI3zeeuutZv9mPPjgg8aYyP7ujxw5YsaOHWt69OhhevfubWbMmGE+++yzNh+Twxhj2nbNBwAAoGPjPTgAAMA6BA4AALAOgQMAAKxD4AAAAOsQOAAAwDoEDgAAsA6BAwAArEPgAAAA6xA4AADAOgQOAACwDoEDAACsQ+AAAADr/B/x0uxn6br6ggAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.arange(999, 10000), moving_average(rewards, 1000))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3OyJMQ1ryRs"
   },
   "source": [
    "## Optimal policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOCvgWWdqYyw"
   },
   "source": [
    "Let's print the q_table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 591,
     "status": "ok",
     "timestamp": 1618321716577,
     "user": {
      "displayName": "Marjon Blondeel",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1bZUZhj06leL74e2Mn-eu1NyztKgDXIMJdj6RgQ=s64",
      "userId": "07230704137495875448"
     },
     "user_tz": -120
    },
    "id": "6pXJnq4KWDtR",
    "outputId": "c83ec5f1-1fc0-45e7-89e9-8d801049f427"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y83ycCLWvBTm"
   },
   "source": [
    "Let's use the learned q_table to define the policy. Name the method `policy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "ywaEmwPltP0f"
   },
   "outputs": [],
   "source": [
    "def policy(state): # remove\n",
    "    return np.argmax(q_table[state])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOr6WI1jvuLm"
   },
   "source": [
    "Let's use an Enum to print out nicely which action to take in which state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "WApVVnR3tk7l"
   },
   "outputs": [],
   "source": [
    "class Action(Enum):\n",
    "\n",
    "    LEFT = 0\n",
    "    DOWN = 1\n",
    "    RIGHT = 2\n",
    "    UP = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 468,
     "status": "ok",
     "timestamp": 1618321750133,
     "user": {
      "displayName": "Marjon Blondeel",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1bZUZhj06leL74e2Mn-eu1NyztKgDXIMJdj6RgQ=s64",
      "userId": "07230704137495875448"
     },
     "user_tz": -120
    },
    "id": "SYewgCE6t1g7",
    "outputId": "58218484-9290-493d-8f81-e1f76254fe2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state:  0  action to take:  LEFT\n",
      "state:  1  action to take:  LEFT\n",
      "state:  2  action to take:  LEFT\n",
      "state:  3  action to take:  LEFT\n",
      "state:  4  action to take:  LEFT\n",
      "state:  5  action to take:  LEFT\n",
      "state:  6  action to take:  LEFT\n",
      "state:  7  action to take:  LEFT\n",
      "state:  8  action to take:  LEFT\n",
      "state:  9  action to take:  LEFT\n",
      "state:  10  action to take:  LEFT\n",
      "state:  11  action to take:  LEFT\n",
      "state:  12  action to take:  LEFT\n",
      "state:  13  action to take:  LEFT\n",
      "state:  14  action to take:  LEFT\n",
      "state:  15  action to take:  LEFT\n"
     ]
    }
   ],
   "source": [
    "for state in range(state_space_size):\n",
    "    print(\"state: \", state, \" action to take: \", Action(policy(state)).name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oNYFkJPcvI1P"
   },
   "source": [
    "Remember the surface looks as follows\n",
    " \n",
    " ```\n",
    "        SFFF\n",
    "        FHFH\n",
    "        FFFH\n",
    "        HFFG\n",
    " \n",
    "    S : starting point, safe\n",
    "    F : frozen surface, safe\n",
    "    H : hole, fall to your doom\n",
    "    G : goal, where the frisbee is located\n",
    "```\n",
    "\n",
    "It seems the best action to state in state 1 (upper left) is to go left. This might seem a strange choice. Why is this?\n",
    "\n",
    "**answer** this is because the agent has learned that it goes down first it might end up somewhere where he didn't want to end up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x7wMyAPgyxYR"
   },
   "source": [
    "## Play FrozenLake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sn98l-X6d8Rw"
   },
   "source": [
    "Let's play FrozenLake for `num_episodes` times and check how well our learned policy does on average. How does it compare to your handcoded policy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "hQXIFZyfe1LL"
   },
   "outputs": [],
   "source": [
    "rewards_test = np.zeros(num_episodes)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()[0]\n",
    "    for step in range(max_steps_per_episode):\n",
    "        state, reward, terminated, truncated, info = env.step(policy(state))\n",
    "        rewards_test[episode] += reward\n",
    "        done = truncated or terminated \n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 491,
     "status": "ok",
     "timestamp": 1618321839062,
     "user": {
      "displayName": "Marjon Blondeel",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1bZUZhj06leL74e2Mn-eu1NyztKgDXIMJdj6RgQ=s64",
      "userId": "07230704137495875448"
     },
     "user_tz": -120
    },
    "id": "D4OQGYudfDUB",
    "outputId": "a2008a62-d20b-43aa-8a22-62aa6bbecbb1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(rewards_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g3uOXaA8fKxe"
   },
   "source": [
    "If you want to play FrozenLake ones and see how the environment evolves you can run the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "pm9GIAYQzDCE"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26612,
     "status": "ok",
     "timestamp": 1618322057094,
     "user": {
      "displayName": "Marjon Blondeel",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1bZUZhj06leL74e2Mn-eu1NyztKgDXIMJdj6RgQ=s64",
      "userId": "07230704137495875448"
     },
     "user_tz": -120
    },
    "id": "1XA645-hzGT7",
    "outputId": "c09846b5-608b-46c9-a118-5f6b976ad908"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fell into a hole ‚ò†Ô∏è\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False, render_mode=\"human\")\n",
    "state = env.reset()[0]\n",
    "print(env.render())\n",
    "for step in range(max_steps_per_episode):\n",
    "    time.sleep(0.1)\n",
    "    clear_output(wait=True)\n",
    "    action = policy(state)       \n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "    env.render()  \n",
    "    done = truncated or terminated \n",
    "    if done:\n",
    "        break\n",
    "\n",
    "if state == 15:\n",
    "    print(\"\\nReached the goal üèÜ\")\n",
    "else:\n",
    "    print(\"\\nFell into a hole ‚ò†Ô∏è\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ah6pzMUjfzqZ"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of 2_ReinforcementLearningIntro_solutions.ipynb",
   "provenance": [
    {
     "file_id": "1_lfz8dLcph3SxvIXFRMEShABtetZPUsU",
     "timestamp": 1640256006360
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
