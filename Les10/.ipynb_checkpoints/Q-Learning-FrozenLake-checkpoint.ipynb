{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_X39pQ5tVP4"
   },
   "source": [
    "# Train an agent to play FrozenLake using Q-learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ss-eZTqjte3O"
   },
   "source": [
    "[Gymnasium](https://gymnasium.farama.org) is a toolkit for developing and comparing reinforcement algorithms. It contains several test problem (*environments*) that have a shared interface, allowing you to write general algorithms. \n",
    "\n",
    "Let's start with importing all necessary dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "K5LVzQUirmiF"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.9 environment at: /Users/juanbenjumea/coding/EHB/aie/.venv\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 17ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install 'gymnasium[classic-control]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WfP0PR7_tlLY"
   },
   "source": [
    "One of the environments Gym contains is FrozenLake.\n",
    "\n",
    "    Winter is here. You and your friends were tossing around a frisbee at the\n",
    "    park when you made a wild throw that left the frisbee out in the middle of\n",
    "    the lake. The water is mostly frozen, but there are a few holes where the\n",
    "    ice has melted. If you step into one of those holes, you'll fall into the\n",
    "    freezing water. At this time, there's an international frisbee shortage, so\n",
    "    it's absolutely imperative that you navigate across the lake and retrieve\n",
    "    the disc. However, the ice is slippery, so you won't always move in the\n",
    "    direction you intend.\n",
    "    The surface is described using a grid like the following\n",
    " \n",
    "        SFFF\n",
    "        FHFH\n",
    "        FFFH\n",
    "        HFFG\n",
    " \n",
    "    S : starting point, safe\n",
    "    F : frozen surface, safe\n",
    "    H : hole, fall to your doom\n",
    "    G : goal, where the frisbee is located\n",
    " \n",
    "    The episode ends when you reach the goal or fall in a hole.\n",
    "    You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
    "\n",
    "More info on: https://gymnasium.farama.org/environments/toy_text/frozen_lake/\n",
    "\n",
    "Let's start and create the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "V90ySjfLtynA"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cgsfBm4At2Lg"
   },
   "source": [
    "## Initialize the Q-table "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6lM_cuat-9q"
   },
   "source": [
    "Remember: \n",
    "- number of rows: number of states\n",
    "- number of columns: number of actions\n",
    "q-table: np array of dimension (states, actions)\n",
    "\n",
    "add some things on how to make np arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WcMatYVCuXM1"
   },
   "outputs": [],
   "source": [
    "state_space_size = env.observation_space.n\n",
    "action_space_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "O7Rr1yo1ujag"
   },
   "outputs": [],
   "source": [
    "q_table = np.zeros((state_space_size, action_space_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1143,
     "status": "ok",
     "timestamp": 1618321123871,
     "user": {
      "displayName": "Marjon Blondeel",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1bZUZhj06leL74e2Mn-eu1NyztKgDXIMJdj6RgQ=s64",
      "userId": "07230704137495875448"
     },
     "user_tz": -120
    },
    "id": "dbuj_-HCunZu",
    "outputId": "e1360b6e-180b-47c1-e7a9-47abe8530f38"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0iZkcf_1vL3Y"
   },
   "source": [
    "## Set the hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zot-2RUGvOl9"
   },
   "source": [
    "In this section we will initialize the hyperparameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Booh7w41vfLw"
   },
   "source": [
    "The first one is the *discount factor*. This is a number in [0,1] indicating how much the agent cares about rewards in the future relative to those in the immediate future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "liMcepGyw3uW"
   },
   "outputs": [],
   "source": [
    "discount_factor = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CthvEAPRy0bg"
   },
   "source": [
    "The second parameter is the *learning rate*. This is a number in [0,1] indicating how quickly the agent will adopt the new (learned) Q-value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GiAiP3b0zRnr"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATMjtQ1wzZT-"
   },
   "source": [
    "Finally we set the necessary parameters to deal with the trade-off between exploration and exploitation. We have to set\n",
    "- *initial exploration rate*: upper bound for the exploration rate and initial exploration rate, we will use this to update the exploration rate\n",
    "- *minimum exploration rate*: lower bound for the exploration rate, by setting it to a value greater than 0, we make sure there is always a probability for exploration\n",
    "- *exploration rate*: probability that the agent will explore, will be updated after each episode, using exponential decay: \n",
    "$$N(t) = N_0 e^{-\\lambda t}$$\n",
    "where $N_0$ is the initial quantity, $N(t)$ is the quantity at time step $t$ and $\\lambda$ is the rate of decay.\n",
    "- *exploration rate decay*: how fast or slow does the exploration rate decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "I9qm0lw2-sn9"
   },
   "outputs": [],
   "source": [
    "initial_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_rate = 1\n",
    "exploration_decay_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1eYlK0-c-45x"
   },
   "source": [
    "We also set number of episodes and maximum number of steps per episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "DFurhi73_AsW"
   },
   "outputs": [],
   "source": [
    "num_episodes = 10000\n",
    "max_steps_per_episode = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aaA0V6O9_TzJ"
   },
   "source": [
    "## Train the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3SrazgwEAaN5"
   },
   "source": [
    "Store the total rewards for each episode. This is for diagnostics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Mmxraf1dekU"
   },
   "source": [
    "If an episode is succesfull this translate to reward 1 if not to reward 0, so we can use this array to check % of successes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "XuCPM2laARJs"
   },
   "outputs": [],
   "source": [
    "rewards = np.zeros(num_episodes) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zy0yUfNqCHP8"
   },
   "source": [
    "In each episode:\n",
    "1.   Reset environment\n",
    "2.   For each step in the episode:\n",
    "  - pick an action $a$ by generating a random float $r$ in $[0,1]$\n",
    "    - if $r > \\epsilon$ we choose the next action by exploitation\n",
    "    - if $r \\leq \\epsilon$ we choose the next action by exploration\n",
    "  - take action $a$ and observe reward $R$ and next state $s'$\n",
    "  - update Q-table \n",
    "  $$ q(s,a) = q(s,a) + \\alpha(R + \\gamma max_{a'} q(s', a') - q(s,a)) $$\n",
    "3. decrease exploration rate proportional to its current value, we will use exponential decay: \n",
    "$$N(t) = N_0 e^{-\\lambda t}$$\n",
    "where $N_0$ is the initial quantity and $N(t)$ is the quantity at time step $t$.\n",
    "\n",
    "\n",
    "\n",
    "with \n",
    "- $\\epsilon$ the exploration rate,\n",
    "- $\\lambda$ the exploration rate decay,\n",
    "- $\\alpha$ the learning rate and \n",
    "- $\\gamma$ the discount factor.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "zOvIxI_A_V9J"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'bool8'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample() \u001b[38;5;66;03m#exploration: random action\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# take a step in the environment: let's store the new state under a new name since we still need to use the old state\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m new_state, reward, terminated, truncated , info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# terminated = True if environment terminates (eg. due to task completion, failure etc.)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# truncated = True if episode truncates due to a time limit or a reason that is not defined as part of the task.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m done \u001b[38;5;241m=\u001b[39m truncated \u001b[38;5;129;01mor\u001b[39;00m terminated \n",
      "File \u001b[0;32m~/coding/EHB/AIE/.venv/lib/python3.12/site-packages/gym/wrappers/time_limit.py:50\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/coding/EHB/AIE/.venv/lib/python3.12/site-packages/gym/wrappers/order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/coding/EHB/AIE/.venv/lib/python3.12/site-packages/gym/wrappers/env_checker.py:37\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43menv_step_passive_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/coding/EHB/AIE/.venv/lib/python3.12/site-packages/gym/utils/passive_env_checker.py:233\u001b[0m, in \u001b[0;36menv_step_passive_checker\u001b[0;34m(env, action)\u001b[0m\n\u001b[1;32m    230\u001b[0m obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m result\n\u001b[1;32m    232\u001b[0m \u001b[38;5;66;03m# np.bool is actual python bool not np boolean type, therefore bool_ or bool8\u001b[39;00m\n\u001b[0;32m--> 233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(terminated, (\u001b[38;5;28mbool\u001b[39m, \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbool8\u001b[49m)):\n\u001b[1;32m    234\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpects `terminated` signal to be a boolean, actual type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(terminated)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    236\u001b[0m     )\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncated, (\u001b[38;5;28mbool\u001b[39m, np\u001b[38;5;241m.\u001b[39mbool8)):\n",
      "File \u001b[0;32m~/coding/EHB/AIE/.venv/lib/python3.12/site-packages/numpy/__init__.py:414\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchar\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mchar\u001b[39;00m\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m char\u001b[38;5;241m.\u001b[39mchararray\n\u001b[0;32m--> 414\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    415\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;18m__name__\u001b[39m, attr))\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'bool8'"
     ]
    }
   ],
   "source": [
    "for episode in range(num_episodes):\n",
    "    state = env.reset()[0] # start with a clean slate\n",
    "    reward_episode = 0 # keep track of total reward for this episode\n",
    "\n",
    "    for step in range(max_steps_per_episode):\n",
    "        # exploration or exploitation? -> we need to pick a random float in 0 and 1\n",
    "        exploration_rate_treshold = random.uniform(0,1)\n",
    "        if exploration_rate_treshold > exploration_rate:\n",
    "            action = np.argmax(q_table[state]) #exploitation: look in row \"state\" which column \"action\" has the highest value\n",
    "        else:\n",
    "            action = env.action_space.sample() #exploration: random action\n",
    "\n",
    "        # take a step in the environment: let's store the new state under a new name since we still need to use the old state\n",
    "        new_state, reward, terminated, truncated , info = env.step(action)\n",
    "        \n",
    "        # terminated = True if environment terminates (eg. due to task completion, failure etc.)\n",
    "        # truncated = True if episode truncates due to a time limit or a reason that is not defined as part of the task.\n",
    "        done = truncated or terminated \n",
    "\n",
    "        # update q-table for current (state, action) pair\n",
    "        q_table[state, action] += learning_rate *\\\n",
    "         (reward + discount_factor * np.max(q_table[new_state]) - q_table[state, action])\n",
    "\n",
    "        # update state\n",
    "        state = new_state\n",
    "        # update reward episode\n",
    "        reward_episode += reward # rewards[episode] += reward\n",
    "\n",
    "        # check if episode ended\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # add reward current episode to rewards array\n",
    "    rewards[episode] = reward_episode\n",
    "\n",
    "    # update exploration rate\n",
    "    exploration_rate = max(initial_exploration_rate * np.exp(- exploration_decay_rate * episode), min_exploration_rate)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IsE0vf_ArDFX"
   },
   "source": [
    "Let's print the average reward per thousand episodes, this way we can get an idea about how rewards have changed over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KduAhbZ3ZuAI"
   },
   "outputs": [],
   "source": [
    "rewards_per_thousand_episodes = np.split(rewards, num_episodes/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after  1000  episodes:  0.259\n",
      "after  2000  episodes:  0.717\n",
      "after  3000  episodes:  0.91\n",
      "after  4000  episodes:  0.965\n",
      "after  5000  episodes:  0.988\n",
      "after  6000  episodes:  0.986\n",
      "after  7000  episodes:  0.993\n",
      "after  8000  episodes:  0.988\n",
      "after  9000  episodes:  0.991\n",
      "after  10000  episodes:  0.992\n"
     ]
    }
   ],
   "source": [
    "count = 1000\n",
    "for reward_group in rewards_per_thousand_episodes:\n",
    "    print(\"after \", count, \" episodes: \", np.average(reward_group))\n",
    "    count+=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THvt3Zx5_O-X"
   },
   "source": [
    "Let's also visualize the *learning curve* by plotting the moving average using a window length of 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BnJK6G4Z-9Yg"
   },
   "outputs": [],
   "source": [
    "def moving_average(array, window):\n",
    "    return np.convolve(array, np.ones(window), 'valid') / window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 800,
     "status": "ok",
     "timestamp": 1618321543335,
     "user": {
      "displayName": "Marjon Blondeel",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1bZUZhj06leL74e2Mn-eu1NyztKgDXIMJdj6RgQ=s64",
      "userId": "07230704137495875448"
     },
     "user_tz": -120
    },
    "id": "C-npvt2E_HIY",
    "outputId": "669283d5-cee7-4a03-a96b-8d2cb7997085"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.259, 0.26 , 0.26 , ..., 0.992, 0.992, 0.992], shape=(9001,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moving_average(rewards, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "executionInfo": {
     "elapsed": 778,
     "status": "ok",
     "timestamp": 1618321555999,
     "user": {
      "displayName": "Marjon Blondeel",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1bZUZhj06leL74e2Mn-eu1NyztKgDXIMJdj6RgQ=s64",
      "userId": "07230704137495875448"
     },
     "user_tz": -120
    },
    "id": "ZYCLeHCm_YwY",
    "outputId": "01d176c1-b242-42f3-8a7b-3631c5a9442b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAOtZJREFUeJzt3Ql8VNXd//Ff9oWskJBAEgg7RPYAMSyij1FUHrdai9YCpUqr1dZKW5UqULVK/62lWEvF+ki1aitaqRsYpFFUSgQFkR3ZEwLZgOxknft/nRNyyUACScjMneXzfr2Ge+/kzuQ3M2HmO+eec66PYRiGAAAAWMTXql8MAACgEEYAAIClCCMAAMBShBEAAGApwggAALAUYQQAAFiKMAIAACxFGAEAAJbyFzdgs9nk6NGjEh4eLj4+PlaXAwAA2kDNq1peXi49e/YUX19f9w4jKogkJSVZXQYAAOiA3NxcSUxMdO8wolpEmh5MRESE1eUAAIA2KCsr040JTZ/jbh1Gmg7NqCBCGAEAwL1cqIsFHVgBAIClCCMAAMBShBEAAGApwggAALAUYQQAAFiKMAIAACxFGAEAAO4VRj799FO5/vrr9dSuatzw22+/fcHbrF27VkaPHi1BQUHSv39/eemllzpaLwAA8PYwUllZKSNGjJAlS5a0af+DBw/K1KlT5YorrpAtW7bIz372M7nrrrtk9erVHakXAAB4mHbPwHrttdfqS1stXbpU+vTpI3/4wx/09pAhQ2TdunXyxz/+UaZMmdLeXw8AADyMw/uMZGdnS0ZGht11KoSo61tTU1Oj57NvfgEAAJ7J4WEkPz9f4uLi7K5T2ypgnDp1qsXbLFy4UCIjI80LZ+wFAMBzueSJ8ubOnStz5sw556x/AAB0hM1mSGF5jV6vqq2X17/IlfoGQwb3CJdv8sulwTD0z0IC/OT+jAES5O9nccXexeFhJD4+XgoKCuyuU9vq7LshISEt3kaNulEXAPAUhmHI4eNVUlFTLyGBfvL06j2S3q+bjO4Vbe5TbzPk79mHpK7BkCsHd5fgAD95f+vRC57xVN33/w7vKYnR9u+p3cICJTw4QMKCXPJ7Z6c5Vdsg+4sq9Pqh45WSuT1fP2cRwf5y29heUllbL7f99fM2399f1u6X60f0NLeD/H1lRnpv6RPTRT+f7VVUXiMFZdXnXP/ml7lyoqpOekYFy/XDG39fUtdQiQw58zvqG2yyt7BCGmyGDIwLl0D/th3QKKuuk7JTdZIYHdriz2vqG2RfYYWczmBa39guEhpozd+Kw39renq6rFq1yu66NWvW6OsBuL66BptsOnxSqusa9LZ6M0ztHX3ON0f1xpYQFaI/aJs+IL8+Uiqrd+TL17klcvmgWP1m2kS9B56oqJV/f5UntubviKc/lKdf2lvCg/1laEKkxIQ578tJ7okqOVhcKaN6RZ3zwbPzaJkUlp/5UOkbEya9urX8Zt/c8YoaueqPn8qJylq76z/Ynt/qbd77+mi76n5/67FWfzY2OVoC/M58iKlsk9orWjJS4mR4YpT+IFePOz4yWAbHR+h91OutXnf1QawCk6/v+QORI5VX18lXOSXm34mfr4+u6VhptazfXyzz39nR6m1f25BzznUBfj468DWtq7+3yQNjdVhUr31Lz/+/Nh3Ry3F9usrGgydkfL9ubar9eEWt7Ckov+B+z39ywFxv/P/V+Hqt33/cbr+m36ueiokDYiSlZ4QsXbtfYsOD5Nupieb/xd+s3KXX4yKCpH/3MLlxRIIkdg2RlVuP6cD2333296us+PF4u3DsTD6Gesdoh4qKCtm3b59eHzVqlCxatEgP2+3atav06tVLH2LJy8uTv//97+bQ3qFDh8q9994rP/jBD+Sjjz6Sn/70p7Jy5co2j6ZRh2lU35HS0lLdogKgbdQb6+bDJ8XXV2RC/xjpHh6sm6s/2l0opafqZFteqX4zH9ozUn/YbM8rlajQALk1NUm/8S/77yHZdazlDuSD4sJbfJNV1ytteQNuq9vH9ZInbxoq+4oq9Jt0725dLvo+80pOyYYDx6W8ul5eWn9IAv18paquXnJPnDrnsShHTlZJZW1jIGsuNNBPenUNtfuGWW+z6ec7e/9x8fXxOe9zER8RbLddUF6t76vpenVf1w7tob+Vt/Yaf7D9mPirF7mZ/Ba+ibdFeJC/9IwKsatZfWD/9lvDG38e7C9XDonT37rV34YKbN+08Piq6hrkb+sO2oUgRf19fWfMuYfdT1bV6uCgXofmck9WSVULz3tLVGj19/WR2gab3lZ/K02viyGGzJrQR+6e3O+89/HZ3iLZW9DYyqJk7sjXQU0Fn4t19mutQqG63/jT13f0NbsY6vXucrrl7P9mjtHhvzO19fO73WFETWCmwsfZZs6cqScz+/73vy+HDh3S+zW/zQMPPCA7d+6UxMREmTdvnt6vsx8M4OnUm+LNf/mv/iaXMSROHrxmkBwtqdYfqk1shshfP92vPwSPn/VNPCYsUIor7K9rK/UhoVpF1GGGjugb00VCg860pmzPOxNyVJP4VSmNHd2/PHRCNuec1Mfzd+e3/CE+aUCMTOwfY3fdp3uLZFhClESHBsgrnx+W0qo6uXVMkv5meDb1GJ79qPFLVUcMTYiwq7+t1OOc/78p+pt1v+5dzFYIRzhackp/C1biIoPtWhn+vv5wp4ZFZ1GhTx2uUEGyucHx4fr/wv8Mth8s0ZlUGN2cU6IPrajgpPqWTE9PbtNt/Xx8ZEL/bhIVGnje/Y6VnpIvD53UrYbNqcNvwf5+OowrxeU18voXOfr/o/oE33G0zPy7bFJV0yCTB8XqVpFH/r3dvF6FyeRuXeTOiX10S6VqWXEkh4URKxBG4M3UMePsA8flzpe+NL/xdQbV1K2ag9UHo3oT69YlSD75pkg36TcdN1bf3Hp3DZVpY3uZb1q/y9wtO4+VyS2jE2X5F7n6fmZNSNYhRzUVBzRrzg8O9JMRiVF6n7OpToTfFFTI0J4R4n/Wt+Hmb86T/t/HOnw5SlqfrvrQkmqeHtM7WsRHJDo0UE6eFeSUAH9f/XjUh0BxRY38/I2v9fWqulnjk/U38c/2FesWA/XOqj44bx2TKLFhjU3l3c/6Zmz139Wjb2/X38xVy8f/u2W47C+q1Nc3Ua+nCnbq8IWy4eBxqa5r+W9QHeY4m/r7Uc+Xeo6VNzcdOedQ1dlUP5rhCZG6NeTl7EM6VKvne97UFPOQ2OHjlZJ38pRuzVP333RoEC1TAVS1gqqWo+aHSp2BMAK4OfVfUx2z//bS1ufkae5boxL0B6niIz4ypEe4TBoQq79N1tbb7JqA1be6pK4hF+wY6SrPgzp0cqquQXfK+8eGHP0h11zO8Sr58vBJ3UR/w8iesr+wQgeda4fFt3q/6jm6ZXSCjD+rhQUXbp1TQUEd9qiorpeIkAB9COnswzFAez6/PbuLNeDG+sy17/itPHHjJfKdsUm686jqYNh0yKRraOB5Oxiqb46Roe0fBeAKVGBq3kl0bHLjt2xYQ432ADobYQRwEappX3UEVJ34jpxobBZv8uTNQ+WOtN5216lhn+oCAO6OMAK4ANUp7pf/2triz/Y+eS1N4AA8GmEEsIjqwPm3/x6Sj3cX6v4OzecRiIsIlm+NTtCdKgkiADwdYQSwgJpA6RdvNo7EaO7jX1ze6nwSAOCpCCOAE6jhjL9ZuVPP+jhtTFKLQeTfPx5PEAHglQgjgAOooaY7jpbK0dJq6R4eJD/551fmz5pPM53188nSp1sXPVTVHYbZAoAjEEaADpyr5ft/26jP/6CmKVfDbNUkYOqcFQeKKuTdLUfPmfm0OXUyKjWR062pidIvNsyptQOAK2LSM6CNEz3d9fKXHZ5CWwWVb49O1HOEAIC3KGPSM+DiqZPKjXj8Q30ytZZcOzTePPOqOj+G6vOhpgBXs4V+f3yyniBKnZOltenOAQCEEaBF6pT3L3x2oMXTsv/mpqFyzdB487T2KrCoc8YwARkAdAxhBDjLnz/aK09/+I3ddaN6RcmbP0pvsYVDTcMe7EsQAYCOIowA55kF9bKBsXLb2CR9OIbRLgDgGIQRQERKqmrPCSJP3zpCn9WVEAIAjkUYAUT0yemafDetl/zquiESFsR/DwBwBt5t4ZUTkv1hzR7ZePCEVNU2SEqPCMk+cFz/7JbRifLUzcOsLhEAvAphBF5FTatzzTOf6hDSpCmIKNOYBwQAnI4wAq/x/Cf7ZeEHu+2u69YlUO6c1Ecfkrk6JV7iI4Mtqw8AvBVhBB7peEWN3PF/GyQiJEAfjjlbUtcQ+eQXV+hhuQAAaxFG4NbniFHnePHz9dETj6nzwTz01lb5aHfheW/36p1pcmnfrgQRAHARhBG4lROVtRIdGiAFZTVy6cIsfd0/Z18qt7/w+QVv2797mGTeP4mp2QHAxRBG4Dae+c9e+eN/7GdGVVoLIurcMGqIboCfD3OFAIALI4zALdTW21oMIs2pc8VcMzRO7prYV5JjujitNgDAxSGMwC3c8Od151x318Q+Um8z5KX1h/T2xl9dST8QAHBDhBG4tJr6Bqmpt8nu/HK9PSIxUu6a1FeuGNzdnCH1jrRe0jMqhCACAG6KMAKXlbk9X+5+dZPdde/cN/Gc/QbEhTuxKgBAZ2NYAVx2ptSzg8jIpCjL6gEAOA4tI3A51XUNMnhept11d0/uJ3dO7GNZTQAAxyGMwOWMfmKN3Uypnz34P5bWAwBwLA7TwOUOzzQ/id2aByZbWg8AwPEII3Apr35+2Fz/7MErJDjAz9J6AACORxiBS3lv6zG9DAnwk6SuoVaXAwBwAsIIXMbvMnebZ9h9+tYRVpcDAHASOrDCJfz2g92y9JP95va4Pl0trQcA4OItI0uWLJHk5GQJDg6WtLQ02bhxY6v71tXVyeOPPy79+vXT+48YMUIyM+2HbcK7O6xmLPrELoj8LGOAxIYHWVoXAMCFw8jy5ctlzpw5smDBAtm8ebMOF1OmTJHCwsIW93/00Ufl+eefl2effVZ27twpd999t9x8883y1VdfdUb9cHNvbc6TfYUV5vYzt42Un2UMtLQmAIBz+Rjqq2k7qJaQsWPHyp///Ge9bbPZJCkpSX7yk5/Iww8/fM7+PXv2lEceeUTuvfde87pbbrlFQkJC5NVXX23T7ywrK5PIyEgpLS2ViIiI9pQLF2azGdL3V6vM7QNPXcf5ZQDAg7T187tdLSO1tbWyadMmycjIOHMHvr56Ozs7u8Xb1NTU6MMzzakgsm7duWdhbX4b9QCaX+B5nv1on7n+6p1pBBEA8FLtCiPFxcXS0NAgcXFxdter7fz8/BZvow7hLFq0SPbu3atbUdasWSMrVqyQY8cah3C2ZOHChTpJNV1Uyws8y65jZfLH/3xjbk8cEGNpPQAADx7a+8wzz8iAAQNk8ODBEhgYKPfdd5/MmjVLt6i0Zu7cubpJp+mSm5vr6DLhROrI4LXPfGZuv/yDcZbWAwBwozASExMjfn5+UlBQYHe92o6Pj2/xNrGxsfL2229LZWWlHD58WHbv3i1hYWHSt2/fVn9PUFCQPrbU/ALP8dh7O+1GzkweGGtpPQAANwojqmUjNTVVsrKyzOvUoRe1nZ6eft7bqn4jCQkJUl9fL2+99ZbceOONHa8abn1G3pfWH9LrPSKDGTkDAGj/pGdqWO/MmTNlzJgxMm7cOFm8eLFu9VCHXpQZM2bo0KH6fSgbNmyQvLw8GTlypF7++te/1gHmwQcf7PxHA5f3+9V7zPV37ptgaS0AADcNI9OmTZOioiKZP3++7rSqQoaaxKypU2tOTo5df5Dq6mo918iBAwf04ZnrrrtOXnnlFYmKiurcRwK36Cvy4rqDen14YqR0D7cfZQUA8E7tnmfECswz4v7qG2wy8NEPxHb6r235Dy+VtL7drC4LAOBu84wAHfXf/cfNIKKMSebcMwCARoQROMVHu86MwHrrnvHixwRnAIDTCCNwihWb8/TyR5f1ldTe0VaXAwBwIYQRONyCd7ZLeU29Xh9NEAEAnIUwAodqsBnycvZhc/vyQUxwBgCwRxiBQ2U16yuy5oHLJMjfz9J6AACuhzACh3rwra162Te2iwyIC7e6HACAJ0x6BrRFTX2D7Mkvl5KqOr19z+R+VpcEAHBRhBF0uvzSarl04ZnzFym3jkmyrB4AgGsjjKDT2GyG/Gz5Fnn366N21/9gQh/LagIAuD76jKDTZB84fk4QUR68ZpAl9QAA3AMtI+gUewvK5Y7/22A3hPelWeMsrQkA4B4II+iUk+Bd9cdPze3Zk/rI3GuHWFoTAMB9cJgGF6WgrFqm/mmduT08MVIHEV/OPQMAaCNaRtBhVbX1kvaU/aiZd++baFk9AAD3RMsIOmzO8q/ttsf362ZZLQAA90UYQYdl7si3235+eqpltQAA3BeHadAhdQ02c/3P3x0lVwzqLl2C+HMCALQfnx7okCUf79PLAD8fuXZoD/GjwyoAoIM4TIN2MwxDFv9nr17v3a0LQQQAcFEII2i3D3cWmOv0EwEAXCzCCNrdKvKjVzbp9ejQAOkXG2Z1SQAAN0cYQbusbjaC5jc3DbO0FgCAZyCMoF1W7zhziGbq8B6W1gIA8AyMpkGbfHnohHx7aba5/ZubhlpaDwDAc9Aygjb57gtnzsirXJUSZ1ktAADPQhjBBTXYDKltNslZz8hgiYsItrQmAIDn4DANLhhEFq3ZY26/OHOMTB4Ya2lNAADPQhhBqwrLq2Xck2fOyhsTFihXDuHwDACgc3GYBq2646x+Io/dQKdVAEDnI4ygRaWn6mRvYYW5rQ7NMJQXAOAIHKZBix7819fm+v6nruP8MwAAh6FlBOew2Qy7yc0IIgAARyKM4By78svM9aXfG21pLQAAz9ehMLJkyRJJTk6W4OBgSUtLk40bN553/8WLF8ugQYMkJCREkpKS5IEHHpDq6uqO1gwHm/qndeb6NUPpJwIAcLEwsnz5cpkzZ44sWLBANm/eLCNGjJApU6ZIYWFhi/v/4x//kIcffljvv2vXLnnxxRf1ffzqV7/qjPrRyUqr6sz174xJtLQWAIB3aHcYWbRokcyePVtmzZolKSkpsnTpUgkNDZVly5a1uP/69etlwoQJ8t3vfle3plx99dVy++23X7A1Bc53qrZBRjz+obn96xsusbQeAIB3aFcYqa2tlU2bNklGRsaZO/D11dvZ2WdOotbc+PHj9W2awseBAwdk1apVct1117X6e2pqaqSsrMzuAse77x+b7bZDAxlsBQBwvHZ92hQXF0tDQ4PExdnPwqm2d+/e3eJtVIuIut3EiRPFMAypr6+Xu++++7yHaRYuXCiPPfZYe0rDRSooq5as3WcOtb1330RL6wEAeA+Hj6ZZu3atPPXUU/KXv/xF9zFZsWKFrFy5Up544olWbzN37lwpLS01L7m5uY4u06uVVddJ2lNnpn1/7a40GZYYaWlNAADv0a6WkZiYGPHz85OCgjNzUChqOz4+vsXbzJs3T6ZPny533XWX3h42bJhUVlbKD3/4Q3nkkUf0YZ6zBQUF6Quc49fv7jDXg/x9ZUL/GEvrAQB4l3a1jAQGBkpqaqpkZZ35Fm2z2fR2enp6i7epqqo6J3CoQKOowzawvtPqis155vbOx6+xtB4AgPdpdw9FNax35syZMmbMGBk3bpyeQ0S1dKjRNcqMGTMkISFB9/tQrr/+ej0CZ9SoUXpOkn379unWEnV9UyiBdZZ+st9c/+zBK5htFQDg+mFk2rRpUlRUJPPnz5f8/HwZOXKkZGZmmp1ac3Jy7FpCHn30UfHx8dHLvLw8iY2N1UHkySef7NxHgg61ijyTtdfcTuoaamk9AADv5GO4wbESNbQ3MjJSd2aNiIiwuhyPUF3XIIPnZZrbf5s1Vq4Y1N3SmgAAnqWtn9+cm8ZLvbPlTD8RZfKAWMtqAQB4N8KIF1KNYQ+9tc3c/uv0VPGlrwgAwCJMsemF1u8/bq7/c/alkt6vm6X1AAC8Gy0jXihze75eRocGEEQAAJYjjHhxf5HvjE2yuhQAAAgj3mbtnkIpq67X69dc0vKsuQAAOBNhxMvM/vuX5vqIxChLawEAQCGMeJFbnlsvdQ2N08rERwQzggYA4BIII16ivLpONh0+aW6/+5MJltYDAEATwoiXSHvqzMkNH752sHQPD7a0HgAAmhBGvEBJVa1U1TaY23dP7mdpPQAANEcY8QJZuwrN9SduvMTSWgAAOBthxAsUlteY69PG9rK0FgAAzkYY8XA19Q3y/zJ36/U7J/aRQH9ecgCAa+GTyYN9uCNfBj2aaW6HBvpZWg8AAC0hjHiwH76yyW57enpvy2oBAKA1hBEvsfKnExnOCwBwSf5WFwDHOFlZa65n/Xyy9IsNs7QeAABaQ8uIh/p0b5G5ThABALgywoiHuv/1LXp5dUqc1aUAAHBehBEPdKz0lLmeMYQwAgBwbYQRDzTlj5+a699OTbS0FgAALoQw4mHqGmxSVl1vbvv6+lhaDwAAF0IY8TD/2Vlgrq/66SRLawEAoC0IIx5mydp95npKzwhLawEAoC0IIx6mvsHQyysHd7e6FAAA2oQw4mF255fr5dThPawuBQCANiGMeIidR8sk+eGV5vYVg2gZAQC4B8KIh7juT5/ZbUd3CbSsFgAA2oMw4gHySs5McqaM6hVlWS0AALQXJ8rzAEvX7jfXZ6b3lkf/N8XSegAAaA/CiAc4Vlqtl1MuiZPHbhxqdTkAALQLh2ncXHl1nfxnV+NEZ7eN62V1OQAAtBthxM091+wQzaC4cEtrAQCgIwgjbq6y5sx5aOIjgi2tBQAAp4WRJUuWSHJysgQHB0taWpps3Lix1X0vv/xy8fHxOecyderUDhUMewVlNXr5QMZATooHAPCOMLJ8+XKZM2eOLFiwQDZv3iwjRoyQKVOmSGFhYYv7r1ixQo4dO2Zetm/fLn5+fnLrrbd2Rv1eL3NHvl4Oig+zuhQAAJwTRhYtWiSzZ8+WWbNmSUpKiixdulRCQ0Nl2bJlLe7ftWtXiY+PNy9r1qzR+xNGLt6JylpzfWhCpKW1AADglDBSW1srmzZtkoyMjDN34Ourt7Ozs9t0Hy+++KLcdttt0qVLl1b3qampkbKyMrsLzvW9/9tgridGh1paCwAATgkjxcXF0tDQIHFxcXbXq+38/MbDBeej+paowzR33XXXefdbuHChREZGmpekpKT2lOkVauobZOexxpAWEcx0MQAA9+XU0TSqVWTYsGEybty48+43d+5cKS0tNS+5ublOq9FdPPivreZ61s8vt7QWAAAuRru+UsfExOjOpwUFjZNsNVHbqj/I+VRWVsrrr78ujz/++AV/T1BQkL6gde9sOWqux4bzXAEAvKRlJDAwUFJTUyUrK8u8zmaz6e309PTz3vbNN9/UfUG+973vdbxaaK98fthc/5/B3S2tBQCAi9XuzgZqWO/MmTNlzJgx+nDL4sWLdauHGl2jzJgxQxISEnS/j7MP0dx0003SrVu3iy7a2817e7u5/uztoyytBQAAp4eRadOmSVFRkcyfP193Wh05cqRkZmaanVpzcnL0CJvm9uzZI+vWrZMPP/zwogv2doZh2G13CaLzKgDAvfkYZ3+6uSA1tFeNqlGdWSMiIsSbvbMlT+5/fYte3/3ENRIc4Gd1SQAAXNTnN+emcTNf5ZToZVRoAEEEAOARCCNu5qX1h/TyoWsGW10KAACdgjDiRg4WV5rrkSEBltYCAEBnIYy4iZKqWrni6bXm9pRLzj+vCwAA7oIw4ib+temIuT4iMVL8fH0srQcAgM5CGHGTwzO/WbnL3H7j7vNPMAcAgDshjLiBpz/cY67//QfjJMifUTQAAM9BGHEDWbsKzLPzXjYw1upyAADoVIQRFzf9xQ1SXWfT66//kMMzAADPQxhxcZ/tLTbXB8WHW1oLAACOQBhxYTuPltltM4IGAOCJCCMu7I0vc831gwuvs7QWAAAchTDiBlO/Kz4+tIoAADwTYcRF2WxnTqY8Njna0loAAHAkwoiLSlmQaa4vuWO0pbUAAOBIhBEX1TScV+keHmxpLQAAOBJhxAXllZwy1x+6ZrCltQAA4GiEERc04bcfmet3T+5raS0AADgaYcTF1DecOTyT0iOCUTQAAI9HGHExR0uqzfX3fzLR0loAAHAGwoiL2XDwuF4OjAsTX2ZcBQB4AcKIi9l0+KReMoIGAOAtCCMu5vUvGqeAH9enq9WlAADgFIQRF5Jfeqa/yLCESEtrAQDAWQgjLmTNrgJz/fJBsZbWAgCAsxBGXMianY1hZGL/GIb0AgC8BmHEhXz6TZFeTrkkzupSAABwGsKIi3jjdMdVJSOFMAIA8B6EERfx4FtbzfUekSGW1gIAgDMRRlzM7eOSrC4BAACnIoy4gH2FFeb6vVf0t7QWAACcjTDiAu59bbO5nhgdamktAAA4G2HEYjabIXsKyvV6SICf1eUAAOB0hBGL5ZedmXX1j9NGWloLAABWIIxYbNexMnP9mqHxltYCAIDbhJElS5ZIcnKyBAcHS1pammzcuPG8+5eUlMi9994rPXr0kKCgIBk4cKCsWrWqozV7lHua9RcBAMAb+bf3BsuXL5c5c+bI0qVLdRBZvHixTJkyRfbs2SPdu3c/Z//a2lq56qqr9M/+9a9/SUJCghw+fFiioqI66zG4tdp6m17GRwRbXQoAAO4RRhYtWiSzZ8+WWbNm6W0VSlauXCnLli2Thx9++Jz91fUnTpyQ9evXS0BAgL5OtaqgUUSwv5RV18vc6wZbXQoAAK5/mEa1cmzatEkyMjLO3IGvr97Ozs5u8TbvvvuupKen68M0cXFxMnToUHnqqaekoaGh1d9TU1MjZWVldhdP9PTqPTqIKGl9ulldDgAArh9GiouLdYhQoaI5tZ2fn9/ibQ4cOKAPz6jbqX4i8+bNkz/84Q/ym9/8ptXfs3DhQomMjDQvSUmeNytpfYNN/vzxPnM7PpLDNAAA7+Tw0TQ2m033F/nrX/8qqampMm3aNHnkkUf04Z3WzJ07V0pLS81Lbu6Zk8h5imOlZ4b0Do4Pt7QWAADcps9ITEyM+Pn5SUFBgd31ajs+vuVhqWoEjeorom7XZMiQIbolRR32CQwMPOc2asSNuniyLbkl5vr7P5loaS0AALhNy4gKDqp1Iysry67lQ22rfiEtmTBhguzbt0/v1+Sbb77RIaWlIOItck5U6WXf2C7i78d0LwAA79XuT0E1rPeFF16Ql19+WXbt2iX33HOPVFZWmqNrZsyYoQ+zNFE/V6Np7r//fh1C1Mgb1YFVdWj1Zh9sP6aXN45IsLoUAADca2iv6vNRVFQk8+fP14daRo4cKZmZmWan1pycHD3CponqfLp69Wp54IEHZPjw4XqeERVMHnroIfFWhmHI9rzGEULJMZwYDwDg3XwM9cno4tTQXjWqRnVmjYiIEHf3SvYhmffODr3+9fyrJTK0cf4VAAA8SVs/v+msYIEVX+WZ6wQRAIC3I4w4WV2DTb7KaRxJk96Xic4AACCMONmh4kpz/dH/HWJpLQAAuALCiJO9tiFHL4cnRsolPSOtLgcAAMsRRpzspfWH9HJbXqnVpQAA4BIIIxaZmc6ZiwEAUAgjTlRTf+ZMxfde0d/SWgAAcBWEESf6eHeRuR4T5r1T4QMA0BxhxInufnWTue7j42NpLQAAuArCiJM0n+h28sBYS2sBAMCVEEacpKiixlx/fnqqpbUAAOBKCCNO8sx/9prrwQF+ltYCAIArIYw4SX2Dy5+PEAAASxBGnCRrd4Fe/nLKIKtLAQDApRBGnNR5tabeptdTerZ+CmUAALwRYcQJjlfWSnl1vV7nTL0AANgjjDjBlpwSvYwJC6LzKgAAZyGMOMGh45V6GR0aYHUpAAC4HMKIE3y4o7Hz6lUpcVaXAgCAyyGMOEH16RPkdQ8PsroUAABcDmHEweoabLL1SKleH98/xupyAABwOYQRB9tbUGGuJ3frYmktAAC4IsKIg+WcqNLLwfHhEujP0w0AwNn4dHSw3NNhpH/3MKtLAQDAJRFGHGzhB7v0snt4sNWlAADgkggjDmY7fX68BlvjdPAAAMAeYcSBKmoap4BX7vufAZbWAgCAqyKMONB/djZOdhYVGiCxzDECAECLCCMO9P7Wo3rZJdDf6lIAAHBZhBEHOlFZq5czx/e2uhQAAFwWYcSBNp8+W296X2ZeBQCgNYQRB9l2egp4pV93Zl4FAKA1hBEH+Wh3oV4GB/hKKH1GAABoFWHEQd473Xn1ltGJVpcCAIDnhZElS5ZIcnKyBAcHS1pammzcuLHVfV966SXx8fGxu6jbebrjFTV6OSIpyupSAADwrDCyfPlymTNnjixYsEA2b94sI0aMkClTpkhhYeNhiZZERETIsWPHzMvhw4fFkx0qrpSTVXV6/frhPa0uBwAAzwojixYtktmzZ8usWbMkJSVFli5dKqGhobJs2bJWb6NaQ+Lj481LXFyceLKX1h/Sy25dAiUk0M/qcgAA8JwwUltbK5s2bZKMjIwzd+Drq7ezs7NbvV1FRYX07t1bkpKS5MYbb5QdO3ac9/fU1NRIWVmZ3cUdz9R7SUKk1aUAAOBZYaS4uFgaGhrOadlQ2/n5+S3eZtCgQbrV5J133pFXX31VbDabjB8/Xo4cOdLq71m4cKFERkaaFxVi3Mmh45V6+cNJfa0uBQAAl+fw0TTp6ekyY8YMGTlypEyePFlWrFghsbGx8vzzz7d6m7lz50ppaal5yc3NFXdR32CT/UWNYSQ5JtTqcgAAcHntmgAjJiZG/Pz8pKCg8QRwTdS26gvSFgEBATJq1CjZt29fq/sEBQXpizvakts466rSIzLE0loAAPC4lpHAwEBJTU2VrKws8zp12EVtqxaQtlCHebZt2yY9evQQT7T0kwN62b97mPj5+lhdDgAALq/dU4OqYb0zZ86UMWPGyLhx42Tx4sVSWVmpR9co6pBMQkKC7vehPP7443LppZdK//79paSkRH7/+9/rob133XWXeKK8klN6OTa5q9WlAADgmWFk2rRpUlRUJPPnz9edVlVfkMzMTLNTa05Ojh5h0+TkyZN6KLDaNzo6WresrF+/Xg8L9jSGYUjeycaRNLeNda9OtwAAWMXHUJ+gLk4N7VWjalRnVjWBmqvKOV4ll/3+Y72++4lrJDiAOUYAAN6rrI2f35ybphN9fuC4Xvr7+hBEAABoI8JIJ8o5PdnZ5YNirS4FAAC3QRjpRGu/aTw/T1qfblaXAgCA2yCMdKLDxY0tIz2jmF8EAIC2Iox0kj355VJeU6/XU3tHW10OAABugzDSSZatO2iux0W45+yxAABYgTDSSQL8G2dbHZoQIT4+zLwKAEBbEUY6yeHjjf1Fbh/Xy+pSAABwK4SRTvJNQble9o0Js7oUAADcCmGkE5SeqpOCshq93rtbqNXlAADgVggjneDdr4+a6z0igy2tBQAAd0MY6QSf7CnSyysGxdJ5FQCAdiKMdILDxyv1Mr0fM68CANBehJGLdKKyVvYWVuj1ay7pYXU5AAC4HcLIRVq57Zi5nhjNNPAAALQXYeQifX7guLnu60t/EQAA2oswcpFWbj3TMgIAANqPMNJJHp06xOoSAABwS4SRi2CzGeb6ZQNjLa0FAAB3RRi5CEdLT5nrzLwKAEDHEEYuwv6ixvlF+sZ2kSB/P6vLAQDALRFGLsJn3zTOvNo3povVpQAA4LYIIxeh5FSdXoYF+VtdCgAAboswchH+temIXl45JM7qUgAAcFuEkQ4qq25sFVGGJ0ZaWgsAAO6MMNJBX+WU6GWAn4/07kafEQAAOoow0kE5J6r0sl9smNWlAADg1ggjHTTv7e16Obp3tNWlAADg1ggjHWAYZ2ZeDQ9mJA0AABeDMNIBxRW15voPJ/W1tBYAANwdYaQDdhwtNecX6RYWZHU5AAC4NcJIB+SVNJ6TJj4y2OpSAABwe4SRDsg53jiSZtKAGKtLAQDA7RFGOuD9rcf0sndXztQLAMDFIox0gI9P47JHVIjVpQAA4J1hZMmSJZKcnCzBwcGSlpYmGzdubNPtXn/9dfHx8ZGbbrpJ3NXh45Vy5GRjn5HRvZhjBAAAp4eR5cuXy5w5c2TBggWyefNmGTFihEyZMkUKCwvPe7tDhw7JL37xC5k0aZK4s3nv7DDXY8ICLa0FAACvDCOLFi2S2bNny6xZsyQlJUWWLl0qoaGhsmzZslZv09DQIHfccYc89thj0reve8/LsfnwSb1MiArRrTwAAMCJYaS2tlY2bdokGRkZZ+7A11dvZ2dnt3q7xx9/XLp37y533nlnm35PTU2NlJWV2V1cZeZVP9/GALL4tpFWlwMAgPeFkeLiYt3KERcXZ3e92s7Pz2/xNuvWrZMXX3xRXnjhhTb/noULF0pkZKR5SUpKEleZX6T0VJ3uwDosIdLqcgAA8AgOHU1TXl4u06dP10EkJqbtc3LMnTtXSktLzUtubq64gqdW7dJLdWqa4AA/q8sBAMAjtOssbypQ+Pn5SUFBgd31ajs+Pv6c/ffv3687rl5//fXmdTabrfEX+/vLnj17pF+/fufcLigoSF9czaptLbf+AAAAJ7WMBAYGSmpqqmRlZdmFC7Wdnp5+zv6DBw+Wbdu2yZYtW8zLDTfcIFdccYVed5XDL21xqrbBXP/llEGW1gIAgNe2jChqWO/MmTNlzJgxMm7cOFm8eLFUVlbq0TXKjBkzJCEhQff7UPOQDB061O72UVFRenn29a4u+0Cxuf7jy89tzQEAAE4KI9OmTZOioiKZP3++7rQ6cuRIyczMNDu15uTk6BE2nno+mqSuDOkFAKAz+RhqvKqLU0N71aga1Zk1IiLCkhoef2+nLPvvQZk9qY88MjXFkhoAAHAnbf389rwmDAdQeU0FEaUXJ8cDAKBTEUbaYE9BubkeFtzuI1sAAOA8CCPt6C+iTB3W09JaAADwNISRNmg6S+/UYT0k0J+nDACAzsQnaxvknmxsGUmMDrG6FAAAPA5hpB0tI4QRAAA6H2GkDbbkluhlYjQjaQAA6GyEkQtosBlSVF6j1xNoGQEAoNMRRi4g7/QhGqVfbJiltQAA4IkIIxew5UjjIZqBcWHi58s08AAAdDbCyAXsPFqml/GRHKIBAMARCCMXkLWrQC8HducQDQAAjkAYuYCaeptepvaOtroUAAA8EmHkPOoabJJX0tiBdVQvwggAAI5AGDmP3BNVemhvkL+vdA8PsrocAAA8EmHkPN7afEQve0aFiC8jaQAAcAjCyHms2dnYebVXV2ZeBQDAUQgj51F4eubV28f1sroUAAA8FmGkFfml1VJSVafXLxsYY3U5AAB4LMJIK3blN052poQG+ltaCwAAnoww0or/nO4vcnVKnNWlAADg0QgjraisqdfLiJAAq0sBAMCjEUZaceT02XovHxRrdSkAAHg0wsgFwkhiNMN6AQBwJMJIC2rqG6SgvFqvJ0Vztl4AAByJMNKCoyXVYhgiIQF+0rVLoNXlAADg0QgjLVi3r1gvE6NDxMeHaeABAHAkwkgLdh1rnGMkKICnBwAAR+PTtgX/2JCjl7emJlldCgAAHo8wcpbqugZpOjIzNrmr1eUAAODxCCNnOXy8SndeVYFkSI9wq8sBAMDjEUbOkrW7cRr4YQmRdF4FAMAJCCNnWb2jMYxEMg08AABOQRg5y8nKWr28fnhPq0sBAMArEEaaqa23yZGTVXp9MuekAQDAdcPIkiVLJDk5WYKDgyUtLU02btzY6r4rVqyQMWPGSFRUlHTp0kVGjhwpr7zyiriirUdKxGaIhAb6SffwIKvLAQDAK7Q7jCxfvlzmzJkjCxYskM2bN8uIESNkypQpUlhY2OL+Xbt2lUceeUSys7Nl69atMmvWLH1ZvXq1uJpvCir0ssFm0HkVAABXDSOLFi2S2bNn60CRkpIiS5culdDQUFm2bFmL+19++eVy8803y5AhQ6Rfv35y//33y/Dhw2XdunXiarbllejld9N6WV0KAABeo11hpLa2VjZt2iQZGRln7sDXV2+rlo8LMQxDsrKyZM+ePXLZZZe1ul9NTY2UlZXZXZxhzc7G1p0+MV2c8vsAAEA7w0hxcbE0NDRIXFyc3fVqOz8/v9XblZaWSlhYmAQGBsrUqVPl2WeflauuuqrV/RcuXCiRkZHmJSnJ8dOyq6BUXFGj14f0iHD47wMAAE4cTRMeHi5btmyRL774Qp588knd52Tt2rWt7j937lwdYJouubm5Dq/x6yOl5vrIpCiH/z4AANDIX9ohJiZG/Pz8pKCgcWKwJmo7Pj6+1dupQzn9+/fX62o0za5du3Trh+pP0pKgoCB9caZ3tuTpZVRogAT4MeIZAABnadenrjrMkpqaqvt9NLHZbHo7PT29zfejbqP6hbiSdXuL9fKaS1oPVQAAwOKWEUUdYpk5c6aeO2TcuHGyePFiqays1KNrlBkzZkhCQoJu+VDUUu2rRtKoALJq1So9z8hzzz0nrqK+wSZ7CxuH9d40KsHqcgAA8CrtDiPTpk2ToqIimT9/vu60qg67ZGZmmp1ac3Jy9GGZJiqo/PjHP5YjR45ISEiIDB48WF599VV9P67iYHGluT42uaultQAA4G18DDWMxMWpob1qVI3qzBoR0fkjXV75/LDMe3u7pPSIkFX3T+r0+wcAwBuVtfHzm56aIvLelqN6GcMU8AAAOB1hRERO1TXo5cT+3awuBQAAr0MYEZHc02fqvWwgZ+oFAMDZvD6MlFfXSUlVnV5PjA61uhwAALyO14eR3BOn9DI6NEDCgto9uAgAAFwkrw8j2/Map4FP6kqrCAAAVvD6MHLg9BwjMWGMpAEAwApeH0aWfrJfL/vGdLG6FAAAvJJXh5H9RY1TwCvp/RjWCwCAFbw6jKzYfMRcn9A/xtJaAADwVl4dRgrLGs8cPKF/NwkO8LO6HAAAvJJXh5Fd+WV6+b203laXAgCA1/LqiTVmpifL1iOlMjwpyupSAADwWl4dRm4dk6QvAADAOl59mAYAAFiPMAIAACxFGAEAAJYijAAAAEsRRgAAgKUIIwAAwFKEEQAAYCnCCAAAsBRhBAAAWIowAgAALEUYAQAAliKMAAAASxFGAACApdzirL2GYehlWVmZ1aUAAIA2avrcbvocd+swUl5erpdJSUlWlwIAADrwOR4ZGdnqz32MC8UVF2Cz2eTo0aMSHh4uPj4+VpfjkslTBbXc3FyJiIiwuhzwmrgcXg/XwuvhPa+HYRg6iPTs2VN8fX3du2VEPYDExESry3B56o+I/9iuhdfEtfB6uBZeD+94PSLP0yLShA6sAADAUoQRAABgKcKIBwgKCpIFCxboJVwDr4lr4fVwLbweriXIBV4Pt+jACgAAPBctIwAAwFKEEQAAYCnCCAAAsBRhBAAAWIow4iIWLlwoY8eO1bPMdu/eXW666SbZs2eP3T7V1dVy7733Srdu3SQsLExuueUWKSgosNsnJydHpk6dKqGhofp+fvnLX0p9fb3dPmvXrpXRo0frntP9+/eXl156ySmP0Z399re/1bP//uxnPzOv4/Vwrry8PPne976nn++QkBAZNmyYfPnll+bPVV/8+fPnS48ePfTPMzIyZO/evXb3ceLECbnjjjv0xE5RUVFy5513SkVFhd0+W7dulUmTJklwcLCelfJ3v/ud0x6ju2hoaJB58+ZJnz599HPdr18/eeKJJ+zOP8Lr4ViffvqpXH/99XpmU/Xe9Pbbb9v93JnP/5tvvimDBw/W+6j/l6tWrWr/A1KjaWC9KVOmGH/729+M7du3G1u2bDGuu+46o1evXkZFRYW5z913320kJSUZWVlZxpdffmlceumlxvjx482f19fXG0OHDjUyMjKMr776yli1apURExNjzJ0719znwIEDRmhoqDFnzhxj586dxrPPPmv4+fkZmZmZTn/M7mLjxo1GcnKyMXz4cOP+++83r+f1cJ4TJ04YvXv3Nr7//e8bGzZs0M/b6tWrjX379pn7/Pa3vzUiIyONt99+2/j666+NG264wejTp49x6tQpc59rrrnGGDFihPH5558bn332mdG/f3/j9ttvN39eWlpqxMXFGXfccYf+v/jPf/7TCAkJMZ5//nmnP2ZX9uSTTxrdunUz3n//fePgwYPGm2++aYSFhRnPPPOMuQ+vh2OtWrXKeOSRR4wVK1aoBGj8+9//tvu5s57///73v/o963e/+51+D3v00UeNgIAAY9u2be16PIQRF1VYWKj/wD755BO9XVJSol9g9Z++ya5du/Q+2dnZ5h+nr6+vkZ+fb+7z3HPPGREREUZNTY3efvDBB41LLrnE7ndNmzZNhyGcq7y83BgwYICxZs0aY/LkyWYY4fVwroceesiYOHFiqz+32WxGfHy88fvf/968Tr1GQUFB+g1UUW+U6vX54osvzH0++OADw8fHx8jLy9Pbf/nLX4zo6Gjz9Wn63YMGDXLQI3NPU6dONX7wgx/YXfetb31Lf2gpvB7OJWeFEWc+/9/5znf030NzaWlpxo9+9KN2PQYO07io0tJSvezatatebtq0Serq6nRTWxPVLNarVy/Jzs7W22qpmsji4uLMfaZMmaJPgrRjxw5zn+b30bRP033AnjoMow6znP2c8Xo417vvvitjxoyRW2+9VR/uGjVqlLzwwgvmzw8ePCj5+fl2z6U6H0ZaWprd66GaotX9NFH7q3Nfbdiwwdznsssuk8DAQLvXQx0yPXnypJMeresbP368ZGVlyTfffKO3v/76a1m3bp1ce+21epvXw1oHnfj8d9Z7GGHERc9SrPomTJgwQYYOHaqvU39Y6g9C/fE0pz7o1M+a9mn+wdf086afnW8f9QF56tQphz4ud/P666/L5s2bdX+es/F6ONeBAwfkueeekwEDBsjq1avlnnvukZ/+9Kfy8ssv2z2fLT2XzZ9rFWSa8/f314G/Pa8ZRB5++GG57bbbdAAPCAjQ4VC9Z6n+Bwqvh7Xynfj8t7ZPe18ftzhrrzd+G9++fbv+pgFrqFNp33///bJmzRrdKQvWB3T1De6pp57S2+rDT/0fWbp0qcycOdPq8rzOG2+8Ia+99pr84x//kEsuuUS2bNmiw4jqTMnrgY6gZcTF3HffffL+++/Lxx9/LImJieb18fHxUltbKyUlJXb7q9Eb6mdN+5w9mqNp+0L7qN7Uqsc1zhyGKSws1KNc1LcFdfnkk0/kT3/6k15XyZ/Xw3nUiICUlBS764YMGaJHKzV/Plt6Lps/1+o1bU6NbFIjCtrzmkH0qLCm1hF1KHL69OnywAMPmK2IvB7Winfi89/aPu19fQgjLkL1QVJB5N///rd89NFHeshcc6mpqbo5VB2nbaKO26k34/T0dL2tltu2bbP7A1Pf7NUHW9Mbudqn+X007dN0H2h05ZVX6udSfeNruqhv5qoZummd18N51CHLs4e6q/4KvXv31uvq/4t682v+XKpDXerYd/PXQ4VHFTSbqP9rqtVFHUtv2kcNmVT9gZq/HoMGDZLo6GiHP053UVVVpfsWNOfn56efS4XXw1p9nPj8d9p7WLu6u8Jh7rnnHj0Ma+3atcaxY8fMS1VVld1QUjXc96OPPtJDSdPT0/Xl7KGkV199tR4erIaHxsbGtjiU9Je//KUe/bFkyRKGkrZR89E0Cq+Hc4dX+/v76yGle/fuNV577TX9vL366qt2QxmjoqKMd955x9i6datx4403tjiUcdSoUXp48Lp16/RIqeZDGdWIAzWUcfr06Xoo4+uvv65/D0NJ7c2cOdNISEgwh/aq4aVq2LoaHdaE18PxI/2++uorfVEf5YsWLdLrhw8fdurzr4b2qv+bTz/9tH4PW7BgAUN73Zn6Y2rpouYeaaL+iH784x/roVbqD+Lmm2/WgaW5Q4cOGddee60eC67eHH7+858bdXV1dvt8/PHHxsiRI43AwECjb9++dr8DbQ8jvB7O9d577+lwp4YnDh482PjrX/9q93M1nHHevHn6zVPtc+WVVxp79uyx2+f48eP6zVbNiaGGWM+aNUu/qTen5mRQw4jVfagPXPWmDntlZWX6/4IK48HBwfrvVs150XwIKK+HY3388cctfmaooOjs5/+NN94wBg4cqN/D1FQFK1eubPfj8VH/dKwhCAAA4OLRZwQAAFiKMAIAACxFGAEAAJYijAAAAEsRRgAAgKUIIwAAwFKEEQAAYCnCCAAAsBRhBAAAWIowAgAALEUYAQAAliKMAAAAsdL/ByJLcUlFfNkRAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.arange(999, 10000), moving_average(rewards, 1000))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3OyJMQ1ryRs"
   },
   "source": [
    "## Optimal policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOCvgWWdqYyw"
   },
   "source": [
    "Let's print the q_table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 591,
     "status": "ok",
     "timestamp": 1618321716577,
     "user": {
      "displayName": "Marjon Blondeel",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1bZUZhj06leL74e2Mn-eu1NyztKgDXIMJdj6RgQ=s64",
      "userId": "07230704137495875448"
     },
     "user_tz": -120
    },
    "id": "6pXJnq4KWDtR",
    "outputId": "c83ec5f1-1fc0-45e7-89e9-8d801049f427"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.94148015, 0.93206532, 0.95099005, 0.94148015],\n",
       "       [0.94148015, 0.        , 0.96059601, 0.95099005],\n",
       "       [0.95099005, 0.970299  , 0.95099004, 0.96059601],\n",
       "       [0.96059601, 0.        , 0.85953805, 0.82676439],\n",
       "       [0.89697895, 0.77342022, 0.        , 0.94148015],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.9801    , 0.        , 0.96059601],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.26341448, 0.        , 0.93978168, 0.35408096],\n",
       "       [0.67501004, 0.71286356, 0.98009996, 0.        ],\n",
       "       [0.9702984 , 0.99      , 0.        , 0.97029896],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.79790466, 0.9899974 , 0.66088121],\n",
       "       [0.98007413, 0.98999973, 1.        , 0.98009997],\n",
       "       [0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y83ycCLWvBTm"
   },
   "source": [
    "Let's use the learned q_table to define the policy. Name the method `policy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ywaEmwPltP0f"
   },
   "outputs": [],
   "source": [
    "def policy(state): # remove\n",
    "    return np.argmax(q_table[state])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOr6WI1jvuLm"
   },
   "source": [
    "Let's use an Enum to print out nicely which action to take in which state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WApVVnR3tk7l"
   },
   "outputs": [],
   "source": [
    "class Action(Enum):\n",
    "\n",
    "    LEFT = 0\n",
    "    DOWN = 1\n",
    "    RIGHT = 2\n",
    "    UP = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 468,
     "status": "ok",
     "timestamp": 1618321750133,
     "user": {
      "displayName": "Marjon Blondeel",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1bZUZhj06leL74e2Mn-eu1NyztKgDXIMJdj6RgQ=s64",
      "userId": "07230704137495875448"
     },
     "user_tz": -120
    },
    "id": "SYewgCE6t1g7",
    "outputId": "58218484-9290-493d-8f81-e1f76254fe2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state:  0  action to take:  RIGHT\n",
      "state:  1  action to take:  RIGHT\n",
      "state:  2  action to take:  DOWN\n",
      "state:  3  action to take:  LEFT\n",
      "state:  4  action to take:  UP\n",
      "state:  5  action to take:  LEFT\n",
      "state:  6  action to take:  DOWN\n",
      "state:  7  action to take:  LEFT\n",
      "state:  8  action to take:  RIGHT\n",
      "state:  9  action to take:  RIGHT\n",
      "state:  10  action to take:  DOWN\n",
      "state:  11  action to take:  LEFT\n",
      "state:  12  action to take:  LEFT\n",
      "state:  13  action to take:  RIGHT\n",
      "state:  14  action to take:  RIGHT\n",
      "state:  15  action to take:  LEFT\n"
     ]
    }
   ],
   "source": [
    "for state in range(state_space_size):\n",
    "    print(\"state: \", state, \" action to take: \", Action(policy(state)).name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oNYFkJPcvI1P"
   },
   "source": [
    "Remember the surface looks as follows\n",
    " \n",
    " ```\n",
    "        SFFF\n",
    "        FHFH\n",
    "        FFFH\n",
    "        HFFG\n",
    " \n",
    "    S : starting point, safe\n",
    "    F : frozen surface, safe\n",
    "    H : hole, fall to your doom\n",
    "    G : goal, where the frisbee is located\n",
    "```\n",
    "\n",
    "It seems the best action to state in state 1 (upper left) is to go left. This might seem a strange choice. Why is this?\n",
    "\n",
    "**answer** this is because the agent has learned that it goes down first it might end up somewhere where he didn't want to end up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x7wMyAPgyxYR"
   },
   "source": [
    "## Play FrozenLake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sn98l-X6d8Rw"
   },
   "source": [
    "Let's play FrozenLake for `num_episodes` times and check how well our learned policy does on average. How does it compare to your handcoded policy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hQXIFZyfe1LL"
   },
   "outputs": [],
   "source": [
    "rewards_test = np.zeros(num_episodes)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()[0]\n",
    "    for step in range(max_steps_per_episode):\n",
    "        state, reward, terminated, truncated, info = env.step(policy(state))\n",
    "        rewards_test[episode] += reward\n",
    "        done = truncated or terminated \n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 491,
     "status": "ok",
     "timestamp": 1618321839062,
     "user": {
      "displayName": "Marjon Blondeel",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1bZUZhj06leL74e2Mn-eu1NyztKgDXIMJdj6RgQ=s64",
      "userId": "07230704137495875448"
     },
     "user_tz": -120
    },
    "id": "D4OQGYudfDUB",
    "outputId": "a2008a62-d20b-43aa-8a22-62aa6bbecbb1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(rewards_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g3uOXaA8fKxe"
   },
   "source": [
    "If you want to play FrozenLake ones and see how the environment evolves you can run the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pm9GIAYQzDCE"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26612,
     "status": "ok",
     "timestamp": 1618322057094,
     "user": {
      "displayName": "Marjon Blondeel",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1bZUZhj06leL74e2Mn-eu1NyztKgDXIMJdj6RgQ=s64",
      "userId": "07230704137495875448"
     },
     "user_tz": -120
    },
    "id": "1XA645-hzGT7",
    "outputId": "c09846b5-608b-46c9-a118-5f6b976ad908"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'bool8'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m clear_output(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m action \u001b[38;5;241m=\u001b[39m policy(state)       \n\u001b[0;32m----> 8\u001b[0m state, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m env\u001b[38;5;241m.\u001b[39mrender()  \n\u001b[1;32m     10\u001b[0m done \u001b[38;5;241m=\u001b[39m truncated \u001b[38;5;129;01mor\u001b[39;00m terminated \n",
      "File \u001b[0;32m~/coding/EHB/AIE/.venv/lib/python3.12/site-packages/gym/wrappers/time_limit.py:50\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/coding/EHB/AIE/.venv/lib/python3.12/site-packages/gym/wrappers/order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/coding/EHB/AIE/.venv/lib/python3.12/site-packages/gym/wrappers/env_checker.py:37\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43menv_step_passive_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/coding/EHB/AIE/.venv/lib/python3.12/site-packages/gym/utils/passive_env_checker.py:233\u001b[0m, in \u001b[0;36menv_step_passive_checker\u001b[0;34m(env, action)\u001b[0m\n\u001b[1;32m    230\u001b[0m obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m result\n\u001b[1;32m    232\u001b[0m \u001b[38;5;66;03m# np.bool is actual python bool not np boolean type, therefore bool_ or bool8\u001b[39;00m\n\u001b[0;32m--> 233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(terminated, (\u001b[38;5;28mbool\u001b[39m, \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbool8\u001b[49m)):\n\u001b[1;32m    234\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpects `terminated` signal to be a boolean, actual type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(terminated)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    236\u001b[0m     )\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncated, (\u001b[38;5;28mbool\u001b[39m, np\u001b[38;5;241m.\u001b[39mbool8)):\n",
      "File \u001b[0;32m~/coding/EHB/AIE/.venv/lib/python3.12/site-packages/numpy/__init__.py:414\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchar\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mchar\u001b[39;00m\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m char\u001b[38;5;241m.\u001b[39mchararray\n\u001b[0;32m--> 414\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    415\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;18m__name__\u001b[39m, attr))\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'bool8'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False, render_mode=\"human\")\n",
    "state = env.reset()[0]\n",
    "print(env.render())\n",
    "for step in range(max_steps_per_episode):\n",
    "    time.sleep(0.1)\n",
    "    clear_output(wait=True)\n",
    "    action = policy(state)       \n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "    env.render()  \n",
    "    done = truncated or terminated \n",
    "    if done:\n",
    "        break\n",
    "\n",
    "if state == 15:\n",
    "    print(\"\\nReached the goal \")\n",
    "else:\n",
    "    print(\"\\nFell into a hole \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ah6pzMUjfzqZ"
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of 2_ReinforcementLearningIntro_solutions.ipynb",
   "provenance": [
    {
     "file_id": "1_lfz8dLcph3SxvIXFRMEShABtetZPUsU",
     "timestamp": 1640256006360
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
